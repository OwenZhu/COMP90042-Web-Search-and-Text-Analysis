{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import models\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import constants as const\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"testing_data.pickle\", \"rb\") as input_file:\n",
    "    testing_data = pickle.load(input_file)\n",
    "\n",
    "with open(\"/mnt/new_vocabulary.pickle\", \"rb\") as input_file:\n",
    "    voc = pickle.load(input_file)\n",
    "\n",
    "emb_mat = np.load(\"/mnt/new_word_embedding_matrix.npy\")\n",
    "\n",
    "rm = models.RnnModel(emb_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tool functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_max_length(lst):\n",
    "    length = max((len(e) for e in lst))\n",
    "    return length\n",
    "\n",
    "def convert_word_to_embedding_index(word, voc):\n",
    "    if word in voc:\n",
    "        return voc[word]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f1_score(p,t):\n",
    "    p_tokens=nltk.word_tokenize(p.lower())\n",
    "    t_tokens=nltk.word_tokenize(t.lower())\n",
    "    common=Counter(p_tokens) & Counter(t_tokens)\n",
    "    num_same=sum(common.values())\n",
    "    if num_same==0:return 0\n",
    "    precision=1.0*num_same / len(p_tokens)\n",
    "    recall=1.0*num_same/len(t_tokens)\n",
    "    f1=(2*precision*recall)/(precision+recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "test_ans = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"model/rnn\")\n",
    "    print(\"load sucessfully\")\n",
    "    \n",
    "    batch_i = 0\n",
    "\n",
    "    pbar = tqdm(total = len(testing_data) + 1)\n",
    "    while batch_i < len(testing_data):\n",
    "        \n",
    "        start = batch_i\n",
    "        end = batch_i + const.BATCH_SIZE\n",
    "\n",
    "        batch_data = testing_data[start: end]\n",
    "        q_list = []\n",
    "        c_list = []\n",
    "        s_list = []\n",
    "        e_list = []\n",
    "        for ins in batch_data:\n",
    "            q_list.append(list(map(lambda x: convert_word_to_embedding_index(x, voc), word_tokenize(ins['question']))))\n",
    "            c_list.append(list(map(lambda x: convert_word_to_embedding_index(x, voc), word_tokenize(ins['text']))))\n",
    "\n",
    "        # padding to a matrix by '0'\n",
    "        max_q = find_max_length(q_list)\n",
    "        for i in q_list:\n",
    "            i.extend([0] * (max_q - len(i)))\n",
    "        batch_q = np.asarray(q_list)\n",
    "\n",
    "        max_c = find_max_length(c_list)\n",
    "        for i in c_list:\n",
    "            i.extend([0] * (max_c - len(i)))\n",
    "        batch_c = np.asarray(c_list)\n",
    "\n",
    "        pred_start_point = tf.argmax(rm.output_layer_1, axis=1)\n",
    "        pred_end_point = tf.argmax(rm.output_layer_2, axis=1)\n",
    "\n",
    "        pred_s, pred_e = sess.run([pred_start_point,pred_end_point], feed_dict={rm.context_input: batch_c,\n",
    "                                                                                rm.question_input: batch_q,\n",
    "                                                                                rm.dropout_keep_prob: 1\n",
    "                                                                                })\n",
    "\n",
    "        for i in range(const.BATCH_SIZE):\n",
    "            answer_dict = dict()\n",
    "            index = start + i\n",
    "            answer_dict[\"id\"] = index\n",
    "\n",
    "            if pred_s[i] > pred_e[i]:\n",
    "                pred_s[i], pred_e[i] = pred_e[i], pred_s[i]\n",
    "            \n",
    "            sent = word_tokenize(testing_data[index][\"text\"])\n",
    "            answer_dict[\"text\"] = \" \".join(sent[pred_s[i]: pred_e[i] + 1])\n",
    "\n",
    "            test_ans.append(answer_dict)\n",
    "        \n",
    "        batch_i += const.BATCH_SIZE\n",
    "        pbar.update(const.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(test_ans)\n",
    "display(df)\n",
    "df.index=df['id']\n",
    "df = df.drop(['id'],axis=1)\n",
    "\n",
    "df.to_csv(\"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
