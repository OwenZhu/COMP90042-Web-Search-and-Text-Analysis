{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import models\n",
    "import constants as const\n",
    "import random\n",
    "\n",
    "\n",
    "with open(\"/mnt/training_data.pickle\", \"rb\") as input_file:\n",
    "    # training data is list of dictionary\n",
    "    training_data = pickle.load(input_file)\n",
    "\n",
    "emb_mat = np.load(\"/mnt/word_embedding_matrix.npy\")\n",
    "rm = models.RnnModel(emb_mat)\n",
    "\n",
    "with open(\"/mnt/vocabulary.pickle\", \"rb\") as input_file:\n",
    "    voc = pickle.load(input_file)\n",
    "\n",
    "\n",
    "def find_max_length(lst):\n",
    "    length = max((len(e) for e in lst))\n",
    "    return length\n",
    "\n",
    "\n",
    "def convert_word_to_embedding_index(word, voc):\n",
    "    if word in voc:\n",
    "        return voc[word]\n",
    "    else:\n",
    "        return random.randint(0, len(voc) - 1)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter('model/train', sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    global_step = 0\n",
    "    np.random.shuffle(training_data)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        batch_i = 0\n",
    "        while batch_i < len(training_data):\n",
    "            start = batch_i\n",
    "            end = batch_i + const.BATCH_SIZE\n",
    "            \n",
    "            batch_data = training_data[start: end]\n",
    "            q_list = []\n",
    "            c_list = []\n",
    "            s_list = []\n",
    "            e_list = []\n",
    "            for ins in batch_data:\n",
    "                q_list.append(list(map(lambda x: convert_word_to_embedding_index(x, voc), ins['question'])))\n",
    "                c_list.append(list(map(lambda x: convert_word_to_embedding_index(x, voc), ins['context'])))\n",
    "                s_list.append(ins['start'])\n",
    "                e_list.append(ins['end'])\n",
    "\n",
    "            # padding to a matrix by '0'\n",
    "            max_q = find_max_length(q_list)\n",
    "            for i in q_list:\n",
    "                i.extend([0] * (max_q - len(i)))\n",
    "            batch_q = np.asarray(q_list)\n",
    "\n",
    "            max_c = find_max_length(c_list)\n",
    "            for i in c_list:\n",
    "                i.extend([0] * (max_c - len(i)))\n",
    "            batch_c = np.asarray(c_list)\n",
    "\n",
    "            max_s = find_max_length(s_list)\n",
    "            for i in s_list:\n",
    "                i.extend([0] * (max_s - len(i)))\n",
    "            batch_s = np.asarray(s_list)\n",
    "\n",
    "            max_e = find_max_length(e_list)\n",
    "            for i in e_list:\n",
    "                i.extend([0] * (max_e - len(i)))\n",
    "            batch_e = np.asarray(e_list)\n",
    "\n",
    "            print(batch_q.shape, batch_e.shape, batch_c.shape, batch_s.shape)\n",
    "            _, loss, summaries = sess.run([rm.opm, rm.loss, rm.merged], feed_dict={rm.context_input: batch_c,\n",
    "                                                                                   rm.question_input: batch_q,\n",
    "                                                                                   rm.label_start: batch_s,\n",
    "                                                                                   rm.label_end: batch_e,\n",
    "                                                                                   rm.dropout_keep_prob: 0.8\n",
    "                                                                                   })\n",
    "            \n",
    "            writer.add_summary(summaries, global_step)\n",
    "\n",
    "            print(\"Epoch:\", epoch, \"loss:\", loss)\n",
    "            batch_i += const.BATCH_SIZE\n",
    "            global_step += 1\n",
    "\n",
    "        save_path = saver.save(sess, \"model/cnn\")\n",
    "        print(\"Model saved in path: %s\" % save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}