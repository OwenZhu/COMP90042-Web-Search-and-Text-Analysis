{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import model\n",
    "import constants as const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_data.pickle\", \"rb\") as input_file:\n",
    "    # training data is list of dictionary\n",
    "    training_data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "training_data = shuffle(training_data, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_mat = np.load(\"/mnt/word_embedding_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/COMP90042-Web-Search-and-Text-Analysis/QA_system/model.py:95: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edm = model.EncoderDecoderModel(emb_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocabulary.pickle\", \"rb\") as input_file:\n",
    "    voc = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_length(lst):\n",
    "    length = max((len(e) for e in lst))\n",
    "    return length\n",
    "\n",
    "def convert_word_to_embedding_index(word, voc):\n",
    "    if word in voc:\n",
    "        return voc[word]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loss is: 30.943066\n",
      "Now loss is: 32.040417\n",
      "Now loss is: 24.466595\n",
      "Now loss is: 21.542305\n",
      "Now loss is: 24.770668\n",
      "Now loss is: 28.011972\n",
      "Now loss is: 26.130234\n",
      "Now loss is: 42.632626\n",
      "Now loss is: 19.736866\n",
      "Now loss is: 34.352356\n",
      "Now loss is: 32.776474\n",
      "Now loss is: 19.166567\n",
      "Now loss is: 29.249428\n",
      "Now loss is: 25.775553\n",
      "Now loss is: 20.924637\n",
      "Now loss is: 31.452778\n",
      "Now loss is: 25.573256\n",
      "Now loss is: 27.604984\n",
      "Now loss is: 32.38619\n",
      "Now loss is: 64.18719\n",
      "Now loss is: 18.718254\n",
      "Now loss is: 227.23085\n",
      "Now loss is: 18.694695\n",
      "Now loss is: 20.38555\n",
      "Now loss is: 35.703964\n",
      "Now loss is: 35.211205\n",
      "Now loss is: 44.26131\n",
      "Now loss is: 56.774353\n",
      "Now loss is: 124.81698\n",
      "Now loss is: 23.11853\n",
      "Now loss is: 91.31108\n",
      "Now loss is: 19.508675\n",
      "Now loss is: 32.067505\n",
      "Now loss is: 38.969643\n",
      "Now loss is: 2527.1365\n",
      "Now loss is: 18.302238\n",
      "Now loss is: 164.44211\n",
      "Now loss is: 25.727829\n",
      "Now loss is: 14.914509\n",
      "Now loss is: 47.663048\n",
      "Now loss is: 78.74882\n",
      "Now loss is: 23.982788\n",
      "Now loss is: 17.425968\n",
      "Now loss is: 22.561623\n",
      "Now loss is: 18.65144\n",
      "Now loss is: 18.103432\n",
      "Now loss is: 16.788227\n",
      "Now loss is: 23.044678\n",
      "Now loss is: 20.85632\n",
      "Now loss is: 80.3996\n",
      "Now loss is: 20.095894\n",
      "Now loss is: 20.828335\n",
      "Now loss is: 18.088312\n",
      "Now loss is: 28.361301\n",
      "Now loss is: 20.457563\n",
      "Now loss is: 18.337343\n",
      "Now loss is: 19.873676\n",
      "Now loss is: 29.408047\n",
      "Now loss is: 20.713392\n",
      "Now loss is: 19.21648\n",
      "Now loss is: 27.374786\n",
      "Now loss is: 20.291973\n",
      "Now loss is: 18.857018\n",
      "Now loss is: 22.467382\n",
      "Now loss is: 88.91069\n",
      "Now loss is: 24.303165\n",
      "Now loss is: 22.652273\n",
      "Now loss is: 47.42479\n",
      "Now loss is: 19.732954\n",
      "Now loss is: 17.813353\n",
      "Now loss is: 29.153355\n",
      "Now loss is: 17.356667\n",
      "Now loss is: 20.93633\n",
      "Now loss is: 79.84593\n",
      "Now loss is: 18.604818\n",
      "Now loss is: 27.526918\n",
      "Now loss is: 28.859398\n",
      "Now loss is: 18.122765\n",
      "Now loss is: 25.218937\n",
      "Now loss is: 92.22272\n",
      "Now loss is: 26.413586\n",
      "Now loss is: 18.06313\n",
      "Now loss is: 30.834333\n",
      "Now loss is: 25.044067\n",
      "Now loss is: 33.847935\n",
      "Now loss is: 19.242783\n",
      "Now loss is: 15.958565\n",
      "Now loss is: 15.235434\n",
      "Now loss is: 15.888546\n",
      "Now loss is: 37.71\n",
      "Now loss is: 17.919722\n",
      "Now loss is: 18.9996\n",
      "Now loss is: 32.214336\n",
      "Now loss is: 28.841263\n",
      "Now loss is: 29.35621\n",
      "Now loss is: 18.854467\n",
      "Now loss is: 21.80288\n",
      "Now loss is: 21.378662\n",
      "Now loss is: 20.081055\n",
      "Now loss is: 25.413555\n",
      "Now loss is: 31.50171\n",
      "Now loss is: 31.552238\n",
      "Now loss is: 38.645084\n",
      "Now loss is: 20.41565\n",
      "Now loss is: 31.151962\n",
      "Now loss is: 31.262512\n",
      "Now loss is: 21.585602\n",
      "Now loss is: 14.511919\n",
      "Now loss is: 32.601826\n",
      "Now loss is: 14.065033\n",
      "Now loss is: 28.316376\n",
      "Now loss is: 46.943302\n",
      "Now loss is: 25.13515\n",
      "Now loss is: 19.030941\n",
      "Now loss is: 18.975695\n",
      "Now loss is: 15.323481\n",
      "Now loss is: 18.532063\n",
      "Now loss is: 16.567867\n",
      "Now loss is: 18.804247\n",
      "Now loss is: 21.521198\n",
      "Now loss is: 35.954857\n",
      "Now loss is: 23.852772\n",
      "Now loss is: 20.036823\n",
      "Now loss is: 20.413464\n",
      "Now loss is: 23.267887\n",
      "Now loss is: 14.665443\n",
      "Now loss is: 21.046211\n",
      "Now loss is: 23.001694\n",
      "Now loss is: 22.919464\n",
      "Now loss is: 19.727577\n",
      "Now loss is: 24.671082\n",
      "Now loss is: 14.65213\n",
      "Now loss is: 59.77814\n",
      "Now loss is: 36.530487\n",
      "Now loss is: 13.599246\n",
      "Now loss is: 63.09578\n",
      "Now loss is: 15.090274\n",
      "Now loss is: 25.97668\n",
      "Now loss is: 18.03564\n",
      "Now loss is: 26.378586\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    batch_i = 0\n",
    "    for epoch in range(10):\n",
    "        while batch_i < len(training_data):\n",
    "            start = batch_i\n",
    "            end = batch_i + const.BATCH_SIZE\n",
    "            q_list = []\n",
    "            c_list = []\n",
    "            s_list = []\n",
    "            e_list = []\n",
    "            for ins in training_data[start: end]:\n",
    "                q_list.append(list(map(lambda x: convert_word_to_embedding_index(x, voc), ins['question'])))\n",
    "                c_list.append(list(map(lambda x: convert_word_to_embedding_index(x, voc), ins['context'])))\n",
    "                s_list.append(ins['start'])\n",
    "                e_list.append(ins['end'])\n",
    "            \n",
    "            # padding to a matrix by '0'\n",
    "            max_q = find_max_length(q_list)\n",
    "            for i in q_list:\n",
    "                i.extend([0] * (max_q - len(i)))\n",
    "            batch_q = np.asarray(q_list)\n",
    "            \n",
    "            max_c = find_max_length(c_list)\n",
    "            for i in c_list:\n",
    "                i.extend([0] * (max_c - len(i)))\n",
    "            batch_c = np.asarray(c_list)\n",
    "            \n",
    "            max_s = find_max_length(s_list)\n",
    "            for i in s_list:\n",
    "                i.extend([0] * (max_s - len(i)))\n",
    "            batch_s = np.asarray(s_list)\n",
    "            \n",
    "            max_e = find_max_length(e_list)\n",
    "            for i in e_list:\n",
    "                i.extend([0] * (max_e - len(i)))\n",
    "            batch_e = np.asarray(e_list)\n",
    "\n",
    "\n",
    "            _, loss = sess.run([edm.opm, edm.loss], feed_dict={edm.context_input: batch_c,\n",
    "                                                              edm.question_input: batch_q,\n",
    "                                                              edm.label_start: batch_s,\n",
    "                                                              edm.label_end: batch_e,\n",
    "                                                              edm.dropout_keep_prob: 0.5\n",
    "                                                              })\n",
    "\n",
    "            print(\"Now loss is:\", loss)\n",
    "            batch_i += const.BATCH_SIZE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
