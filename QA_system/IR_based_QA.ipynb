{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-based QA system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing_data.pickle\", \"rb\") as input_file:\n",
    "    testing_data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_mat = np.load(\"word_embedding_matrix.npy\").astype(np.float)\n",
    "\n",
    "with open(\"vocabulary.pickle\", \"rb\") as input_file:\n",
    "    voc = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word, voc, e_mat):\n",
    "    if word in voc:\n",
    "        return e_mat[voc[word], :]\n",
    "    else:\n",
    "        return e_mat[0, :]\n",
    "\n",
    "def get_tokenize_sentences(documents):\n",
    "    tokens = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        sents = nltk.sent_tokenize(doc)\n",
    "        for sent in sents:\n",
    "            sent = sent.strip(\".\")\n",
    "            sent = re.sub(r'[,;\":\\']', '', sent)\n",
    "            tokens.extend(nltk.word_tokenize(sent) )\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def get_sent_embedding(sent, voc, emb_mat):\n",
    "    sent_embedding = np.zeros((len(sent), 50))\n",
    "    for i, word in enumerate(sent):\n",
    "        word_embedding = get_word_embedding(word, voc, emb_mat)\n",
    "        sent_embedding[i, :] = word_embedding\n",
    "\n",
    "    sent_embedding = np.mean(sent_embedding, axis=0)\n",
    "    return sent_embedding\n",
    "    \n",
    "def cos_sim(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare similarity between sentences by average word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ans = []\n",
    "\n",
    "for t in testing_data:\n",
    "    ans = dict()\n",
    "    tokenize_sentences = get_tokenize_sentences(t['text'])\n",
    "    tokenize_question = get_tokenize_sentences([t['question']])\n",
    "    q_emb = get_sent_embedding(tokenize_question[0], voc, emb_mat)\n",
    "\n",
    "    sims = np.zeros((len(tokenize_sentences)))\n",
    "    for i, sent in enumerate(tokenize_sentences):\n",
    "        s_emb = get_sent_embedding(sent, voc, emb_mat)\n",
    "        sims[i] = cos_sim(q_emb, s_emb)\n",
    "    \n",
    "    print(sims)\n",
    "    sentences = []\n",
    "    for para in t['text']:\n",
    "        sentences.extend(nltk.sent_tokenize(para))\n",
    "    \n",
    "    ans[\"id\"] = t['id']\n",
    "    ans['question'] = t['question']\n",
    "    ans[\"text\"] = sentences[np.argmax(sims)]\n",
    "    test_ans.append(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunck & NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(tree):\n",
    "    \"recursively traverses an nltk.tree.Tree to find named entities\"\n",
    "\n",
    "    items = []\n",
    "\n",
    "    if hasattr(tree, 'label') and tree.label:\n",
    "        if tree.label() == \"NP\":\n",
    "            items.append(' '.join([child[0] for child in tree]))\n",
    "        else:\n",
    "            for child in tree:\n",
    "                items.extend(traverse(child))\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_query(ent_lst, q):\n",
    "    q = nltk.word_tokenize(q)\n",
    "    return True in list(map(lambda x: x in q, ent_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handwriting rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ans = []\n",
    "\n",
    "for t in tqdm(testing_data):\n",
    "    answer = dict()\n",
    "    answer[\"id\"] = t['id']\n",
    "    \n",
    "    query = t[\"question\"].lower()\n",
    "    \n",
    "    doc = nlp(t[\"text\"])\n",
    "    answer['text'] = set()\n",
    "    if check_query([\"who\", \"organization\"], query):\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in {\"ORG\", \"PERSON\", \"NORP\"}:\n",
    "                answer['text'].add(ent.text)\n",
    "    elif check_query([\"when\", \"time\", \"month\", \"day\", \"year\"], query):\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in {\"DATE\", \"TIME\", \"CARDINAL\"}:\n",
    "                answer['text'].add(ent.text)\n",
    "    elif check_query([\"where\", \"place\", \"city\", \"country\"], query):\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in {\"GPE\", \"LOC\", \"FACILITY\", \"ORG\"}:\n",
    "                answer['text'].add(ent.text)\n",
    "    elif check_query([\"how much\", \"how many\"], query):\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in {\"PERCENT\", \"QUANTITY\", \"CARDINAL\", \"MONEY\"}:\n",
    "                answer['text'].add(ent.text)\n",
    "    else:\n",
    "        for ent in doc.ents:\n",
    "            if not ent.label_:\n",
    "                answer['text'].add(ent.text)\n",
    "\n",
    "    if not answer['text']:\n",
    "        for chunk in doc.noun_chunks:\n",
    "            answer['text'].add(ent.text)\n",
    "\n",
    "    # delete the entities which already appear in query\n",
    "    answer[\"text\"] = \" \".join(list(answer[\"text\"] - set(nltk.word_tokenize(query))))\n",
    "    # replace Punctuation with empty string\n",
    "    answer[\"text\"] = re.sub(r'[^\\w\\s]', '', answer[\"text\"])\n",
    "    \n",
    "    test_ans.append(answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(test_ans)\n",
    "display(df)\n",
    "df.index=df['id']\n",
    "df = df.drop(['id'],axis=1)\n",
    "\n",
    "df.to_csv(\"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
