{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Preprocessing and Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Due date</b>: 11pm, Sunday March 18th\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -20% per day\n",
    "\n",
    "<b>Marks</b>: 5% of mark for class\n",
    "\n",
    "<b>Overview</b>: In this homework, you'll be using a corpus of tweets to do tokenisation of hashtags and build polarity classifers using bag of word (BOW) features.\n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn, and Gensim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python built-in packages, but do not use any other 3rd party packages (the packages listed above are all fine to use); if your iPython notebook doesn't run on the marker's machine, you will lose marks.  \n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a few minutes, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). The amount each section is worth is given in parenthesis after the instructions. You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
    "\n",
    "<b>Extra credit</b>: Each homework has a task which is optional with respect to getting full marks on the assignment, but that can be used to offset any points lost on this or any other homework assignment (but not the final project or the exam). We recommend you skip over this step on your first pass, and come back if you have time: the amount of effort required to receive full marks (1 point) on an extra credit question will be substantially more than earning the same amount of credit on other parts of the homework.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Instructions</b>: For this homework we will be using the tweets in the <i>twitter_samples</i> corpus included with NLTK. You should start by accessing these tweets. Use the <i>strings</i> method included in the NLTK corpus reader for <i>twitter_samples</i> to access the tweets (as raw strings). Iterate over the full corpus, and print out the average length, in characters, of the tweets in the corpus. (0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average length is 103.85176666666666\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "tweets = twitter_samples.strings()\n",
    "lengths = list(map(lambda x: len(x), tweets))\n",
    "print(\"the average length is\", sum(lengths) / float(len(lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Hashtags (i.e. topic tags which start with #) pose an interesting tokenisation problem because they often include multiple words written without spaces or capitalization. You should use a regular expression to extract all hashtags of length 8 or longer which consist only of lower case letters (other than the # at the beginning, of course, though this should be stripped off as part of the extraction process). Do <b>not</b> tokenise the entire tweet as part of this process. The hashtag might occur at the beginning or the end of the tweet; you should double-check that you aren't missing any. After you have collected them into a list, print out number of hashtags you have collected: for full credit, you must get the exact number that we expect.  (1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['athabasca', 'jaspernationalpark', 'explorealberta', 'snapchat', 'kikhorny', 'tagsforlikes', 'webcamsex', 'snapchat', 'instagram', 'kiksexting']\n",
      "1238\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tweet_hashtags = filter(lambda x : x, \n",
    "                     map(lambda tweet: re.findall(\\\n",
    "                         r'(?:^|\\s)#([a-z]{8,})(?:\\s|$)',\n",
    "                         tweet), tweets))\n",
    "\n",
    "hashtags = [hashtag for tweet in tweet_hashtags for hashtag in tweet]\n",
    "\n",
    "print(hashtags[0:10])\n",
    "print(len(hashtags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Now, tokenise the hashtags you've collected. To do this, you should implement a reversed version of the MaxMatch algorithm discussed in class (and in the reading), where matching begins at the end of the hashtag and progresses backwards. NLTK has a list of words that you can use for matching, see starter code below. Be careful about efficiency with respect to doing word lookups. One extra challenge you have to deal with is that the provided list of words includes only lemmas: your MaxMatch algorithm should match inflected forms by converting them into lemmas using the NLTK lemmatiser before matching. Note that the list of words is incomplete, and, if you are unable to make any longer match, your code should default to matching a single letter. Create a new list of tokenised hashtags (this should be a list of lists of strings) and use slicing to print out the last 20 hashtags in the list. (1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['scot', 'night'], ['democrats'], ['worrying'], ['falling', 'labour'], ['leaders', 'debate'], ['wow', 'campaign'], ['tory', 'lies'], ['election'], ['bias', 'e', 'd', 'b', 'b', 'c'], ['labour', 'doorstep'], ['bias', 'e', 'd', 'b', 'b', 'c'], ['b', 'b', 'c', 'debate'], ['mil', 'if', 'and', 'om'], ['u', 'k', 'parliament'], ['bedroom', 'tax'], ['canna', 'b', 'is'], ['vote', 'green'], ['l', 'lane', 'l', 'li', 'husting', 's'], ['bedroom', 'tax'], ['bankrupt']]\n"
     ]
    }
   ],
   "source": [
    "words = nltk.corpus.words.words() # words is a Python list\n",
    "words = set(words)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def max_match(hashtag, wordlist):\n",
    "    lst = []\n",
    "    for i in range(len(hashtag) - 1, -1, -1):\n",
    "        first_word = hashtag[0:i+1]\n",
    "        remainder = hashtag[i+1:len(hashtag)]\n",
    "        if WordNetLemmatizer().lemmatize(first_word) in wordlist:\n",
    "            lst.append(first_word)\n",
    "            if remainder != '':\n",
    "                lst += max_match(remainder, wordlist)\n",
    "            return lst\n",
    "    \n",
    "    lst += hashtag[0]\n",
    "    return lst\n",
    "\n",
    "print(list(map(lambda x: max_match(x, words), hashtags[-20:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit (Optional)\n",
    "<b>Instructions</b>: Implement the forward version of the MaxMatch algorithm as well, and print out all the hashtags which give different results for the two versions of MaxMatch. Your main task is to come up with a good way to select which of the two segmentations is better for any given case, and demonstrate that it works significantly better than using a single version of the algorithm for all hashtags. (1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max match result is: ['scot', 'night']\n",
      "forward version is: ['t', 'scot', 'nigh']\n",
      "the better one is:\n",
      "['scot', 'night']\n",
      "max match result is: ['democrats']\n",
      "forward version is: ['s', 'democrat']\n",
      "the better one is:\n",
      "['democrats']\n",
      "max match result is: ['worrying']\n",
      "forward version is: ['g', 'worry', 'in']\n",
      "the better one is:\n",
      "['worrying']\n",
      "max match result is: ['falling', 'labour']\n",
      "forward version is: ['r', 'falling', 'lab', 'o', 'u']\n",
      "the better one is:\n",
      "['falling', 'labour']\n",
      "max match result is: ['leaders', 'debate']\n",
      "forward version is: ['e', 'leaders', 'deb', 'at']\n",
      "the better one is:\n",
      "['leaders', 'debate']\n",
      "max match result is: ['wow', 'campaign']\n",
      "forward version is: ['n', 'wow', 'camp', 'ai', 'g']\n",
      "the better one is:\n",
      "['wow', 'campaign']\n",
      "max match result is: ['tory', 'lies']\n",
      "forward version is: ['s', 'tory', 'lie']\n",
      "the better one is:\n",
      "['tory', 'lies']\n",
      "max match result is: ['election']\n",
      "forward version is: ['n', 'elect', 'io']\n",
      "the better one is:\n",
      "['election']\n",
      "the better one is:\n",
      "['bias', 'e', 'd', 'b', 'b', 'c']\n",
      "max match result is: ['labour', 'doorstep']\n",
      "forward version is: ['p', 'labour', 'doors', 'te']\n",
      "the better one is:\n",
      "['labour', 'doorstep']\n",
      "the better one is:\n",
      "['bias', 'e', 'd', 'b', 'b', 'c']\n",
      "max match result is: ['b', 'b', 'c', 'debate']\n",
      "forward version is: ['e', 'b', 'b', 'c', 'deb', 'at']\n",
      "the better one is:\n",
      "['b', 'b', 'c', 'debate']\n",
      "max match result is: ['mil', 'if', 'and', 'om']\n",
      "forward version is: ['m', 'mil', 'if', 'and', 'o']\n",
      "the better one is:\n",
      "['mil', 'if', 'and', 'om']\n",
      "max match result is: ['u', 'k', 'parliament']\n",
      "forward version is: ['t', 'u', 'k', 'par', 'li', 'amen']\n",
      "the better one is:\n",
      "['u', 'k', 'parliament']\n",
      "max match result is: ['bedroom', 'tax']\n",
      "forward version is: ['x', 'bedroom', 'ta']\n",
      "the better one is:\n",
      "['bedroom', 'tax']\n",
      "max match result is: ['canna', 'b', 'is']\n",
      "forward version is: ['s', 'canna', 'b', 'i']\n",
      "the better one is:\n",
      "['canna', 'b', 'is']\n",
      "max match result is: ['vote', 'green']\n",
      "forward version is: ['n', 'vote', 'gree']\n",
      "the better one is:\n",
      "['vote', 'green']\n",
      "the better one is:\n",
      "['l', 'lane', 'l', 'li', 'husting', 's']\n",
      "max match result is: ['bedroom', 'tax']\n",
      "forward version is: ['x', 'bedroom', 'ta']\n",
      "the better one is:\n",
      "['bedroom', 'tax']\n",
      "max match result is: ['bankrupt']\n",
      "forward version is: ['t', 'bank', 'r', 'up']\n",
      "the better one is:\n",
      "['bankrupt']\n"
     ]
    }
   ],
   "source": [
    "def max_match_forward(hashtag, wordlist):\n",
    "    lst = []\n",
    "    for i in range(len(hashtag) - 1, -1, -1):\n",
    "        remainder = hashtag[0:i+1]\n",
    "        last_word = hashtag[i+1:len(hashtag)]\n",
    "        if WordNetLemmatizer().lemmatize(last_word) in wordlist:\n",
    "            lst.append(last_word)\n",
    "            if remainder != '':\n",
    "                lst += max_match(remainder, wordlist)\n",
    "            return lst\n",
    "    \n",
    "    lst += hashtag[0]\n",
    "    return lst\n",
    "\n",
    "for hashtag in hashtags[-20:]:\n",
    "    max_match_result = max_match(hashtag, words)\n",
    "    max_match_forward_result = max_match_forward(hashtag, words)\n",
    "    if set(max_match_result) != set(max_match_forward_result):\n",
    "        print(\"max match result is:\", max_match_result)\n",
    "        print(\"forward version is:\", max_match_forward_result)\n",
    "    # select the algorithm that generates less tokens:\n",
    "    print(\"the better one is:\")\n",
    "    if len(max_match_result) <= len(max_match_forward_result):\n",
    "        print(max_match_result)\n",
    "    else:\n",
    "        print(max_match_forward_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification (Not Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: The twitter_sample corpus has two subcorpora corresponding to positive and negative tweets. You can access already tokenised versions using the <i> tokenized </i> method, as given in the code sample below. Iterate through these two corpora and build training, development, and test sets for use with Scikit-learn. You should exclude stopwords (from the built-in NLTK list) and tokens with non-alphabetic characters (this is very important you do this because emoticons were used to build the corpus, if you don't remove them performance will be artificially high). You should randomly split each subcorpus, using 80% of the tweets for training, 10% for development, and 10% for testing; make sure you do this <b>before</b> combining the tweets from the positive/negative subcorpora, so that the sets are <i>stratified</i>, i.e. the exact ratio of positive and negative tweets is preserved across the three sets. (1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def exclude_stopwords(tweets, stop_words):\n",
    "    tokenized_tweets = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        tokenized_tweet = []\n",
    "        \n",
    "        for token in tweet:\n",
    "            token = token.lower()\n",
    "            if not token in stop_words and not re.search(r'[^a-z]', token):\n",
    "                tokenized_tweet.append(token)\n",
    "        \n",
    "        tokenized_tweets.append(tokenized_tweet)\n",
    "        \n",
    "    return tokenized_tweets\n",
    "\n",
    "def split_dataset(X, y, test_size):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "def prepare_data(tweets, classifications):\n",
    "    feature_matrix = []\n",
    "    for tweet in tweets:\n",
    "        feature_dict = get_BOW(tweet)   \n",
    "        feature_matrix.append(feature_dict)\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = split_dataset(feature_matrix, \n",
    "                                                     classifications, \n",
    "                                                     0.2)\n",
    "    \n",
    "    test_X, dev_X, test_y, dev_y = split_dataset(test_X, test_y, 0.5)\n",
    "    \n",
    "    vectorizer = DictVectorizer()\n",
    "    train_X = vectorizer.fit_transform(train_X)\n",
    "    test_X = vectorizer.transform(test_X)\n",
    "    dev_X = vectorizer.transform(dev_X)\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y, dev_X, dev_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_tweets = nltk.corpus.twitter_samples.tokenized(\"positive_tweets.json\")\n",
    "negative_tweets = nltk.corpus.twitter_samples.tokenized(\"negative_tweets.json\")\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "positive_targets = [1 for i in range(len(positive_tweets))]\n",
    "negative_targets = [0 for i in range(len(negative_tweets))]\n",
    "targets = np.asarray(positive_targets + negative_targets)\n",
    "\n",
    "\n",
    "positive_tweets = exclude_stopwords(positive_tweets, stop_words)\n",
    "negative_tweets = exclude_stopwords(negative_tweets, stop_words)\n",
    "\n",
    "train_X, train_y, test_X, test_y, dev_X, dev_y = \\\n",
    "    prepare_data(positive_tweets + negative_tweets, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Now, let's build some classifiers. Here, we'll be comparing Naive Bayes and Logistic Regression. For each, you need to first find a good value for their main regularisation (hyper)parameters, which you should identify using the scikit-learn docs or other resources. Use the development set you created for this tuning process; do <b>not</b> use crossvalidation in the training set, or involve the test set in any way. You don't need to show all your work, but you do need to print out the accuracy with enough different settings to strongly suggest you have found an optimal or near-optimal choice. We should not need to look at your code to interpret the output. (1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb fbeta scores [0.73150796894156112, 0.73355128729055985, 0.73355128729055985, 0.73595735957359576, 0.73679901760130984]\n",
      "lr fbeta scores: [0.32751091703056762, 0.72962728995578019, 0.758948799275034, 0.70764681030840726, 0.69217900460058557, 0.75187969924812026, 0.75497915701713758, 0.74989163415691373, 0.72488342518016102, 0.70472605604349647]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XecHVX9//HXO70QSNh0UoGEaqSE\nJoL0XhQBAQt8LSgKfgXFikr5+lNRwMZXQaT5FQJEwRCaCAQFgSS0hFADpJHeyyab7O7n98c5dzN7\nc3d3tt07u/t5Ph7z2Dtzz8x8Zu7dOfecOWeOzAznnHMuazqVOgDnnHOuEM+gnHPOZZJnUM455zLJ\nMyjnnHOZ5BmUc865TPIMyjnnXCZ5BlUCkq6UZJLeqeP92fH9Kxu53VFxvVMSy74t6YgCaU3SxY2N\nvTVJOiLGtXcj15siaWIDaW6P276pwHvTJd1ejFibKu4rN1VLWijpHkmji7H/lpL4HHLHMVfSbZIG\nteA+DpX0kqRNkrwfTRvmGVTpbAJGSxqfXCjpAGBkfL8lfBs4osDyQ4D7WmgfLeUlQlzvtuI+LpC0\nUwtspxix5rsu7vNQ4FvAfsBDkroUMYaW8CbhOD4K/A9wMjBJUktdj24CVgPHx/24NqqtfbHbkw2E\ni9w5wPTE8nOAJ4H9W3PnZvZ8a26/KcxsLdCacb0O9AcuB77RnA0VIdZC5iQ+t+ckrQYeAsYSjq2t\n2JA4jv9I2gj8mfCdn9aUDUoS0N3MNgG7Azeb2dPNCVJSZ6CzmW1uznZc03kJqrQmAGfHf67cP9nZ\ncXkthaqxGqpmkjQHKAN+nKhWOSK+V6uKL7d9SefFKsa1kh6RNCxvm/0l3SFphaTyuF5+KXCOpF9K\n+q6kRZLWSLpOwUmSZklaJ+kBSf3qOx5J35Q0LW5jiaQHJe2a8vzm2whcD1woaWBdiSTtLmmCpPnx\nGGdJ+kbyF35+rJKelnRvgW39UtK8xGfcQ9K1cdsVkl6VdFITj2dd/Ns1sb+TJT0uaWn8DJ+XdFzi\n/b1i3B/Li3M7SeslfT2x7KPxuMrj5/1HSX0S7/eVdEusbtwUj/OPTTiOF+PfUYltn65Q9bpJ0uJ4\nzpLHeaWk5THGaYQah7NilV5n4NfxOG+P6TvHdebF8z5L0nl55+D2uM+PS5oVt3mQpAvitvaL3/dy\nSa/E+d4KVZRrJL0n6dy8bdb7eeQdy77x/XJJL0s6LP9ESfqSpJnxvCxR+J/dIfF+vZ9ZW+MZVGn9\nDRhEqOoAOAwYANzfQtv/BLAG+BOhquMQQqmtLgcBFwPfBC4kVCHdnJfmAULVybeATxG+Q08VyDTO\nAQ4E/gu4FriMkDlcA/wQ+ArwMeCnDRzDMOB3wOnAlwgXn2eT/5SN9L+EC89l9aTZCXgL+CpwEvBH\n4CrgO/WsMwE4RVLv3IKYKZ0F3Gtbnyk2EbgA+H/AqYQSwyRJ+6SIvZOkLpK6ShobY3oHeC2RZjTw\nIPBZ4JPAf4BHJB0KYGazCCW//8rb9lmEjO6uGPuhwBPAYuBMQonzJOC2xDrXE767lxK+E98HmnLP\nZ1T8uzju+2zC/8ZU4LR4nBey7XelF3AHcAtwAjCDrVV6uerQa+L81cAPCN/n04Bngb/kZygxlmvj\nvk4C3k+8dwdwN+G8ivBZ/glYSDhHLwB3qvaPuno/jwLHclNMVwHcL6lXLoGkK+L7TwMfBy4i/H9v\nF99P85m1LWbmU5En4EpgeXz9d+DG+Pp/gQfi6+XAlYl1pgAT87ZzBOGCsHecHxXnT0mkqbWdxHID\nLs7b/hqgX2LZN2K6nnH+hDj/sUSa3sAy4KbEsjnAbEL1SG7ZVKASGJ1Ydi2wpK7jKRBzZ6AnoeTw\nufrOTYF1bwemJ87/2tyxEqpYb69jPRGqwr8PvFfPuR8Qj++cRJpDYprxcf7o/PMXl/8LuK+B+K3A\nNB/4UD3rdIqxPwbcmlj+RWA9sF1eDBMT8/8Gnsrb3lF5x/wacEkjv/u3x/PdBegG7APMBOYRLtIC\n5gK35a33eUIJuCzxGRpweorv9o6EKvUf56V7GHgrLzYD9slLd0Fcfn5i2UlxWfK87gBsAS5q5OeR\nO5ajEsv2ictOiPN9gXLg+nrObYOfWVubvARVehOAMyV1J/zq2aZ6r4immdmqxHzuvkauUcGBwDJL\n1O2b2QZgMltLgTlTzKwqMT+bcA/l/bxlAyR1qysgSQfHKpIVhAygnPCLcWwjjivfr+Pfrxd6M1bD\nXSVpNuGX7BbgJ4RGLQXv25rZMsK9w08lFn8KeNfMcvcYjyH8un02loS6xO09AdSqJq3DL4AD4nQy\nocTwsBKNPiQNU6iC/YBwvrYAx1H7fOW+Y2fFdXYhfH63xflehMz13rw4n4nby90ffQW4XNJXY4ku\nrf3jdiqAl+OyM82sPMY5osC+nwR6AMnqbAMeSbG/vQmZX36joHuAsapd3fuBmb1Sx3aeSLyeHf8+\nWROM2RrCj7XGfh7E5VMS87n/vVxp7BDCj7OCpaFGfGZtimdQpTeJcMH9CaE08mAJY1mdN5+7Odwj\n/h0CLCmw3hLCr9SGtlVomQi/pLchaQTwj5jmy4TWawcASxMxNVrMhH8PfF3SdgWS/JxQhXkz4Zfy\nAYTWZjSw3wnAiZK2V7hfdRbhIpjTHxhMuGAkpyuB4SlCn2dm0+P0MHBGjOdSgLjPScBHgB8BR8bY\nH0nGbWbrgXvZWs13ASHjfDTO9yOUVv83L84KQjVgLtaLCVW+PwLekvSOpHNSHMcbMa79gEFm9iEz\nmxrf6x//Ppy379wPm+R5WmXpGjAMiX/zv7u5+X4FlhWS/P5uLrAst7wHpP88orVmVp2bSRxXLl1Z\n/LuojtjSfmZtirfiKzEz2yBpMuEic18skRSyiW0v5PmZQmtbBBRqXDAIWNkK+zuB8Mv39Nx5ib8K\nW+K4rwMuIdxnyncW8Fszuza3QNLJKbZ5PyHjO51QTTWU2hnUSuADwv2DZjOzCknvAXvERbsC+wIn\nmlkus0FSzwKr30IoyY0BPgfcmSjxriaUTq4kZBT5Fsb9ryaUQr8uaRyhS8NfJM0ws/paFZYnSpX5\nct+jC9laukpKlsDT3u/KXdQHAisSy3N9r5Lf3ZbsN9WYz6MhubiHEKrt86X6zNoaz6Cy4fdAd+AP\n9aRZAByet+zYFNuu+UXXAl4ArpJ0uJn9C2qqFk6m5Rp2JPUEqglVIzln0wLfWzNbqtDi7DJqX7Ry\n+63IzSg0N26wZGBmqyT9g1C1Nxd4w8xmJJI8QWiAst7M3mzmISCpB7ALWy/kuQtfMvaRhJJnMg7M\n7D+S3gRuJVSp3Z54b4Ok54HdzOzqNLGY2QxJlwOfJjTzbmqz97cImfgoM2tKi8BCXiNUDZ9FaCyR\nczbwdqyebQ2pP48UniPcgzufULqvpSmfWVvgGVQGmNkUatc/F3I/8AVJNxD6vhxJaDnVkDeBkyU9\nSrgx/paZrWtgnbrifEzSs8A9kr5LuLB/i/CP+IumbLMBTxKqLW6T9Cdgr7i//GqVpvoFoTXhIGr3\nv3kc+Fq8B7US+BrhB0Qa9xAu+msIrQ+THifcIH9c0s+BWcD2hBviPczsew1se5Skg+PrAYTS3w6E\nlmQQPusFwHWSfgj0IbSA+6CO7f2JcA6eK5Bhfht4QlI1obXaOkJGdjLwAzN7W9IzhO/la4Rf718i\nNEaYShOZWbWkbwJ/lrQ9oTpsM7AzoeSZu1fVmG2ulPQr4ApJlYRGGmcQqm/zW/G1pMZ+HnUys9WS\nrgF+Eu/ZPkz4Tp4MXGVmH5DiM2uBYyoqvwfVRpjZQ4SWZGcSLgojSdfZ9HLCReMhwkW4uTdLP0G4\n0P6KcNNZhNZHs+tdqwnMbCbhPslBhIYY5xF+Ba9poe0vIDTtzXcJoUXUjYTM5jUabg6f83dCia8/\neQ1eLDSrOiNu8xuEzOomws3tZ1Js+5uEX9LPEW6W9wCOM7NpcfsVcfuVhAvUNTHuujqsPhD/3pr/\nhpk9QyixDyB0on2QcAGcz9b7NM8R7l9NJNzT6k+ozlqQ4ljqZGb3EKpJ9yF8x/5GyIxfYuu9n8b6\nEeFcXET4Lh0OfMbMWq1RUhM+j4a2l4v/GML37CZC67518f00n1mbotgU0TnXwUj6KqGp/1ALT8Zw\nLlO8is+5DkbSKEIz5+8T+n955uQyyUtQznUwCo//OY9Q1XR2Xt835zLDMyjnnHOZ5I0knHPOZVKH\nuAfVv39/GzVqVKnDcM45B7z44ovLzWxAQ+k6RAY1atQopk+vq+O6c865YpI0N006r+JzzjmXSR2i\nBOWcc67xNm2pYv7KcuasKGfuig3MWbGBuSvKOXTX/nzlY7u0+v49g3LOuQ5sQ0Ulc2syoNoZ0eK1\nm0g29N6hZ1dGlfWiSycVJTbPoJxzrp1bs3HL1gxo+QbmrtyaIS1bV1Erbf/tujGyrDeH7FLGqLLe\njCzrVfO3b686h25rFZ5BOedcG2dmrCrfEks+G5izvHaJaFX5llrpB2/fg5FlvThqt4GMSGRAI8t6\n0adH1xIdxbY8g3LOuTbAzFi2roI5K8q3ZkQxA5q7vJx1FVtHpZFg6A49GdW/Fyd+aAijynoxsqw3\no8p6M2LHXvTs1rmER5KeZ1DOOZcR1dXGorWbmLt82/tBc1eUs3FLVU3azp3E8H49GVnWm/1H9AsZ\nUP+QEQ3r15PuXdpGJlQfz6Ccc66IKquqWbh60zaloDkrypm3spzNlTUjv9Otc6dYBdeLQ3ftH6vh\nejOqrBdD+/aka+f23VPIMyjnnGthFZVVLFi1seD9oAWrNlJZvbVpXM+unRlZ1otdBvTm6N0H1mRA\nI/v3ZvD2PehcpBZzWeQZlHPONcHGzVXMW1n7ftC8eH9o4eqNJPIg+nTvwsj+vdhrpx04edyQmvtB\no8p6MaBPd6SOmwnVp1UzKEknAL8mDNt9i5n9LO/9GwhDlwP0AgaaWV9JIwmjaHYGugK/NbM/xHW6\nEYbSPgKoJgxl/NfWPA7nXMe0vqKSOcs3bM2IlpfX6iOU1K9XV0aW9Wb8yH6M3G9Yreq4HXt380yo\nCVotg5LUmTBk9rHAAmCapElm9noujZldmkh/CbBvnF0EfMTMKiRtB7wW110I/ABYamZjJXUCdmyt\nY3DOtX9rYvPsXMaztVHCBpavrz3CfP/tutfcD8pVw40q68XIHXuzQ6/sNM9uLxrMoBSy/U8DO5vZ\n1ZJGAIPNbGoDqx4IzDaz9+J2JgCnA6/Xkf5c4McAZpb8VnSn9jMDPw/sHtNVA8sbOgbnnCvfXMkT\nbyzlnSXrwv2g2Fl1dV4foSE7hD5Cx+wxaOv9oLLejCjrxXbd/a5IMaU52/9LqEo7CrgaWAf8FTig\ngfV2AuYn5hcABxVKGKv0RgNPJpYNBx4CdgUuN7OFkvrGt6+RdATwLnCxmS0psM0LgQsBRowY0UCo\nzrn26vWFa7l76jweePkD1lVU0kmwU7+ejCrrzSnjhtT0DRrVP/zt0bXtN89uL9JkUAeZ2X6SXgYw\ns1XxPlBDClW41jV87znARDOraeRvZvOBcZKGAg9ImghUAcOAZ83sMkmXAb8EPrvNjsxuBm4GGD9+\nvA8b7FwHUr65kskzFnHXC/N4Zf5qunXpxCkfGsI5B45gn+F96dalfTfPbi/SZFBb4v0kA5A0gFCi\nasgCYHhifhiwsI605wBfK/RGLDnNAg4jlNzKgfvj2/cBX0gRi3OuA3hz8VruemEe978USku7DtyO\nH56yJ5/cb6eiP0fONV+aDOo3hAxhoKSfAGcCV6RYbxowRtJo4ANCJnRefiJJuwH9gOcSy4YBK8xs\no6R+wKHA9WZmkh4ktOB7Ejiauu9pOec6gI2bq5g8YyF3TZ3Hy/NCaemkvQdz3kEjOWBUP28914Y1\nmEGZ2V8kvUjIDAR83MzeSLFepaSLgccIzcVvNbNZkq4GppvZpJj0XGCCWfKh7uwBXCfJ4j5/aWYz\n43vfAf4s6VfAMuC/Uh2pc65deWvxOu56YS5/e/kD1m2qZOcBvbni5D345H7D6NfbS0vtgWrnC3lv\nhmbcM8xs7+KF1PLGjx9vPuS7c23fpi1VTJ6xiLunzuPFuavo1rkTJ35oMOcdOIIDR+/opaU2QtKL\nZja+oXT1lqDMrFrSq5JGmNm8lgvPOefSe3vJOu56YR5/e2kBazdVsnP/UFo6Y79h7OilpXYrzT2o\nIcAsSVOBDbmFZnZaq0XlnOvwNm2p4uGZoSXe9FhaOmHvwZx74AgO3tlLSx1BmgzqqlaPwjnnoneW\nrOOuqfP420sfsGbjFkb37833T9qdT+43jLLtupc6PFdEaRpJPC1pEFs75k41s6WtG5ZzriPZtKWK\nR14LpaVpc1bRtbM4fq/BnHfQCA7ZucxLSx1UmkcdnQ38AphCaFH3W0mXm9nEVo7NOdfOzV66Ptxb\nenkBq8u3MKqsF987cXfO3N9LSy5dFd8PgANypabYUfefgGdQzrlG27SlikdfW8xdU+cx9f2VdO0s\njttrMJ8+cAQH71xGpw48/pGrLU0G1SmvSm8FtR/e6pxzDZq9dD0Tps7jry8tYFX5FkaW9eK7sbTU\n30tLroA0GdSjkh4D7o7znwIeab2QnHPtRUVlLC29MI8X3l9Jl07h3tK5B47gI7t4acnVL00jicsl\nnQF8lHAP6mYzu7+B1ZxzHdh7y9Zz99R5THwxlJZG7NiLb5+wG2ftP5wBfby05NJJ00hiNPCwmf0t\nzveUNMrM5rR2cM65tqOisorHZi3hrhfm8vx7obR07J6DOO+gERy6S38vLblGS1PFdx/wkcR8VVzW\n0HhQzrkO4P3lG5gwdR73vbiAlRs2M3zHnlx+/G6cNX4YA/v0KHV4rg1Lk0F1SY5wa2abU44H5Zxr\npzZXVvPYrMXcPXUe/3l3BZ07iWP3CKWlj+7qpSXXMtJkUMsknZZ7+rik0/Fh1p3rkOYs38Dd0+Yx\ncfoCVmzYzE59e/Kt48Zy9vjhDNzeS0uuZaXJoL4C/EXS7wiNJOYDn2vVqJxzmbG5sprHX1/CXVPn\n8uzsUFo6Zo+BnHvgCA4bM4DOXlpyrSRNK753gYMlbUcYnmNd64flnCu1uSs2cPfU+Ux8cT7L14fS\n0jePHcvZBwxnkJeWXBGkacX338BtwDrgj5L2A75rZv9o7eCcc8W1pSqUlu6eOo9/v7Oczp3EUbsP\n5LyDRnC4l5ZckaWp4vu8mf1a0vHAQMIItrcBnkE5107MW1HOhGnzuHf6Apavr2DoDj247Nhwb2nw\nDl5acqWRJoPK/WQ6CbjNzF6VP1rYuTZvS1U1T7yxhL+8EEpLnQRH7T6ITx80gsPHemnJlV6aDOpF\nSf8ARgPfk9QHqE6zcUknAL8GOgO3mNnP8t6/ATgyzvYCBppZX0kjgb/F9boCvzWzP+StOwnYua0P\nR+9csc1fubW0tGxdBUN26ME3jhnDpw4YzpAdepY6POdqpMmgvgDsA7xnZuWSygjVfPWS1Bm4ETgW\nWABMkzTJzF7PpTGzSxPpLwH2jbOLgI+YWUVsnPFaXHdhTHsGsD7VETrnYmlpKXdNnce/31mGgKN2\nDy3xjthtoJeWXCalacVXDbyUmF9BeKJ5Qw4EZpvZewCSJgCnA6/Xkf5c4MdxH5sTy7uTeHp6zLAu\nAy4E7k0Rh3Md1oJV5UyYOp97p89n6boKBm/fg68fFUpLQ/t6acllW5oSVFPtROgzlbMAOKhQwlil\nNxp4MrFsOPAQsCtwea70BFwDXAeU17dzSRcSMjFGjBjRtCNwrg2qrKrmiTeXcvfUeTz99jIAjtxt\nIOcdOIIjdhtAl84+Wo5rG1ozgypUZ2B1pD0HmGhmVTUJzeYD4yQNBR6QNBEYAuxqZpdKGlXfzs3s\nZuBmgPHjx9e1X+fajQWryrl32nzumT6fJWsrGLR9dy6JpaWdvLTk2qDUGZSkgUBNe1Mzm9fAKguA\n4Yn5YcDCOtKeA3yt0BtmtlDSLOAwYACwv6Q5hNgHSppiZkekOQbn2pvKqmqeemsZd70wlymxtHTE\n2AFcc/oIjtp9oJeWXJuWpqPuaYQqtaHAUmAk8AawVwOrTgPGxOE6PiBkQucV2P5uQD/gucSyYcAK\nM9soqR9wKHC9mU0Efh/TjAIme+bkOqKFqzcyYdp87p02n8VrNzGwT3cuPnJXPnXAcIb161Xq8Jxr\nEWlKUNcABwP/NLN9JR1JaNBQLzOrlHQx8BihufitZjZL0tXA9NzDZ+O2JphZshpuD+A6SUaoKvyl\nmc1Mf1jOtT+VVdVMeWsZd02dx5S3lmLA4WMGcNXpe3G0l5ZcO6Ta+UKBBNJ0Mxsv6VVgXzOrljTV\nzA4sTojNN378eJs+fXqpw+hwKquq+c+7K5g8YyFvLPJHODbXkrWbWLquggF9uvOp8cP51AHDGb6j\nl5Zc2yPpRTMb31C6NCWo1bFp978ITzVfClQ2N0DXPlVVGy+8v4LJMxbx6GuLWblhM9t178J+I/vR\nxfvaNMuIsl6cOm4IR+8xiK5eWnIdQJoM6nRgI3Ap8GlgB+Cq1gzKtS3V1cZL81YxecYiHpq5iGXr\nKujZtTNH7zGQUz88lI+NHUCPrp1LHaZzro1Jk0H9yMy+Q3i80R0Akn4OfKc1A3PZZma8umANk19d\nyEMzF7FozSa6denEUbsN5JQPD+Go3QfSq1tr9mJwzrV3aa4gx7JtZnRigWWunTMzXl+0lskzFjF5\nxkLmr9xI187i8DED+PYJu3HMHoPo06NrqcN0zrUTdWZQki4CvgrsLGlG4q0+wLOtHZjLjneWrOPB\nVxcyecYi3lu+gc6dxKG79ueSo8Zw/J6D2aGXZ0rOuZZXXwnqLuAR4KfAdxPL15nZylaNypXc+8s3\nMDlmSm8tWYcEB48u4wuHjeaEvQZTtl33UofonGvn6sygzGwNsAY4V9JHgTFmdpuk/pJGm9n7RYvS\nFcX8leU11XezFq4FYPzIflx12l6c+KHBDOzjA9c554onzZMkfgyMB3YjjKTbDfg/wtMdXBu3aM1G\nHpqxiMkzFvHK/NUAfHh4X644eQ9O+tAQf+K1c65k0jSS+ARhnKaXoObZeH1aNSrXqpatq+CR1xbx\n4KsLmTZnFQB7Dtme75ywO6eMG+KdP51zmZAmg9psZhYfO4Sk3q0ck2sFKzds5tHXFjN5xkKef28F\n1QZjB23HZceO5ZRxQ9h5wHalDtE552pJk0HdK+kmoK+kLwGfB/7YumG5lrBm4xb+MWsxD85YxLOz\nl1NVbYzu35uvHbkrp4wbym6DvSDsnMuuNCPq/lLSscBaYCyh4+7jrR6Za5L1FZX88/UlTJ6xkH+9\nvZzNVdUM69eTLx22M6eMG8JeQ7dH8kcOOeeyL21X/5lAT8KAg/5U8YzZuLmKJ99cyuQZC3nyzaVU\nVFYzePsefPaQkZz64aF8eNgOnik559qcNK34vgj8iDAcu4DfSrrazG5t7eBc3TZtqeLpt5cxecYi\nnnhjCeWbq+i/XXfOOWA4p3x4KPuP6Ecnfzirc64NS1OCupwwzMYKAEllwH8Az6CKbHNlNc/OXs6D\nMxby+KwlrKuopF+vrpy+z06cOm4IB+1cRmfPlJxz7USaDGoBkBzMZx0wv3XCcfkqq6p57r0VTH51\nEY/OWsyajVvo06MLJ+w9mFM+PJSP7FLmQy8459ql+p7Fd1l8+QHwgqS/E+5BnQ5MLUJsHVZVtTFt\nzkomz1jIIzMXs2LDZnp368yxew7ilHFDOWxsf7p38eErnHPtW30lqFwb5HfjlPP31gun4zIzXpq3\nmgdfXcjDMxexdF0FPbp24ujdB3Hqh4dwxG4DfUwl51yHUl8GtauZfVbSf5vZr5uycUknAL8GOgO3\nmNnP8t6/ATgyzvYCBppZX0kjgb/F9boCvzWzP0jqBdwH7AJUAQ+aWfJBtm2KmTHzgzVhoL8Zi/hg\n9Ua6denEEWMHcMqHh3L07gPp3d3HVHLOdUz1Xf32jxnF5yXdSWjBV6OhJ5pL6gzcSBhPagEwTdIk\nM3s9sY1LE+kvITxSCWAR8BEzq4jDzb8maRKwGvilmT0lqRvwhKQTzeyRtAdcambGm4vXMXlGeFL4\n3BXldOkkDhvTn8uOHcuxew1iex9TyTnn6s2g/gA8CuwMvEjtDMri8vocCMw2s/cAJE0g3L96vY70\n5wI/BjCzzYnl3YFOcXk58FQujaSXgGENxJEJs5eu48FXw5PC3122gU6CQ3ftz1eP2IXj9xpM317d\nSh2ic85lSn3DbfwG+I2k35vZRU3Y9k7Ubu23ADioUMJYUhtN6GuVWzYceAjYFbjczBbmrdMXOJVQ\nhVhomxcCFwKMGDGiCeE339wVG5g8IzyU9c3FYUylA0ftyAWHjubEvQfT38dUcs65OqV51FFTMifI\nqxLMba6OtOcAE82sKrHf+cA4SUOBByRNNLMlAJK6AHcDv8mV0ArEfTNwM8D48ePr2m+LW7CqvGb4\nipkfrAFg/5H9+PGpe3LSh4YwaHsfU8k559JozTvwC4DhiflhwMI60p4DfK3QG3F4j1nAYcDEuPhm\n4B0z+1ULxdosS9ZuipnSQl6aF8ZUGjdsB75/0u6cPG4oO/mYSs4512itmUFNA8ZIGk3oS3UOcF5+\nIkm7Af2A5xLLhgErzGyjpH6EwRGvj+/9D7AD8MVWjL1By9dX8MjMRTw4YxHT5qzEDHYf3IfLj9+N\nU8YNYWSZj0rinHPN0WoZlJlVSroYeIzQXPxWM5sl6WpguplNiknPBSaYWbIabg/gujgGlQgt92bG\njOsHwJvAS/EBqL8zs1ta6ziSVpfnxlRaxH/eXU61wa4Dt+O/jx7DKeOGsutAH1PJOedaimrnC+3T\n+PHjbfr06U1ad+2mLfxjVhi+4pl3llNZbYws68Wp44ZyyoeHsNugPv6kcOecawRJL5rZ+IbSeS/Q\nepgZp/32GeasKGenvj35wkdHc8q4oey9k4+p5Jxzrc0zqHpI4vsn7UH/Pt3Zd3hfz5Scc66IPINq\nwHF7DS51CM451yH5OA3OOecyqUM0kpC0DJjbjE30B5a3UDitJesxZj0+yH6MWY8PPMaWkPX4oPkx\njjSzAQ0l6hAZVHNJmp6mxUl+YdRNAAAgAElEQVQpZT3GrMcH2Y8x6/GBx9gSsh4fFC9Gr+JzzjmX\nSZ5BOeecyyTPoNK5udQBpJD1GLMeH2Q/xqzHBx5jS8h6fFCkGP0elHPOuUzyEpRzzrlM8gzKOedc\nJnkGlSDpBElvSZot6bsF3r9M0uuSZkh6Io4EnKX4viJppqRXJD0jac9ixpcmxkS6MyWZpKI2p01x\nDi+QtCyew1ckFX1YlzTnUNLZ8bs4S9JdWYtR0g2Jc/i2pNUZi2+EpKckvRz/n08qZnwpYxwZrzMz\nJE2JozkUM75bJS2V9Fod70vSb2L8MyTt1+JBmJlP4T5cZ+BdYGegG/AqsGdemiOBXvH1RcA9GYtv\n+8Tr04BHs3YOY7o+wL+A54HxWYoPuIAwhEuWv4djgJeBfnF+YNZizEt/CWG4nczER7jJf1F8vScw\nJ2vnELgPOD++Pgr4c5FjPBzYD3itjvdPAh4hDIl0MPBCS8fgJaitDgRmm9l7ZrYZmACcnkxgZk+Z\nWXmcfZ4wSnCW4lubmO0NFLsFTIMxRtcA1wKbihkc6eMrpTQxfgm40cxWAZjZ0gzGmHQucHdRIgvS\nxGfA9vH1DtQ92ndrSRPjnsAT8fVTBd5vVWb2L2BlPUlOB+604Hmgr6QhLRmDZ1Bb7QTMT8wviMvq\n8gXCr4diSRWfpK9JepeQAXy9SLHlNBijpH2B4WY2uZiBRWk/40/GKouJkoYXJ7QaaWIcC4yV9Kyk\n5yWdULTogtT/K7EafDTwZBHiykkT35XAZyQtAB4mlPKKKU2MrwKfjK8/AfSRVFaE2NJq7DWz0TyD\n2qrQWBoFSyCSPgOMB37RqhHl7bbAsm3iM7MbzWwX4DvAFa0eVW31xiipE3AD8M2iRVRbmnP4IDDK\nzMYB/wTuaPWoaksTYxdCNd8RhNLJLZL6tnJcSan/V4BzgIlmVtWK8eRLE9+5wO1mNoxQVfXn+P0s\nljQxfgv4mKSXgY8BHwCVrR1YIzTme9AknkFttQBI/loeRoFiv6RjCMPOn2ZmFUWKDVLGlzAB+Hir\nRrSthmLsA+wNTJE0h1BvPamIDSUaPIdmtiLxuf4R2L9IseWk+ZwXAH83sy1m9j7wFiHDKpbGfBfP\nobjVe5Auvi8A9wKY2XNAD8IDUIslzXdxoZmdYWb7Eq45mNma4oXYoMZekxqvmDfdsjwRfpW+R6iO\nyN203Csvzb6EG5tjMhrfmMTrU4HpWYsxL/0UittIIs05HJJ4/Qng+aydQ+AE4I74uj+hmqUsSzHG\ndLsBc4gPBMhSfITq+Qvi6z0IF9aixZkyxv5Ap/j6J8DVxTyPcb+jqLuRxMnUbiQxtcX3X+wDzvJE\nKOq/HTOhH8RlVxNKSxCqfJYAr8RpUsbi+zUwK8b2VH2ZQ6lizEtb1Awq5Tn8aTyHr8ZzuHvWzmG8\nIFwPvA7MBM7JWoxx/krgZ8WOLeU53BN4Nn7OrwDHZTDGM4F3YppbgO5Fju9uYBGwhVBa+gLwFeAr\nie/hjTH+ma3xv+yPOnLOOZdJfg/KOedcJnkG5ZxzLpM8g3LOOZdJnkE555zLJM+gnHPOZZJnUM45\n5zLJMyjnnHOZ1KXUATjn6idpL0In7BHAn4GBhKdITytpYM61Mu+o61yGSeoBvAScRXg0zpvAi2Z2\nRkkDc64IvATlXLYdA7xsZrMAJHUDrittSM4Vh9+Dci7b9iWUoJA0FFhvZs+WNiTnisMzKOeyrYKt\nIzf/lPDka+c6BM+gnMu2u4DDJb1FePL2c5J+VeKYnCsKbyThnHMuk7wE5ZxzLpM8g3LOOZdJnkE5\n55zLJM+gnHPOZZJnUM455zLJMyjnnHOZ5BmUc865TPIMyjnnXCZ5BuWccy6TPINyzjmXSZ5BOeec\nyyTPoJxzzmWSZ1AdgKQrJS0v0r4ukGSStkuZfmyMr29ztlPP9i0xbZT0hqTvSGqXg3VKul3S9CLv\nL3d+qyXNlXSbpEEtuI9DJb0kaZMkf7p1B9Iu/0ldST0EHAKUp0w/FvgxcDuwuhnbqc91wESgJ3AK\n8DOgK/A/LbDtrLmGcJzF9CbwX4QfvHsBPwH2lHSImVW3wPZvApYCxxPGx3IdhGdQrkWZ2TJgWVa2\nE80xs+fj66ck7QV8jiJlUJJ6mtnGYuzLzN4txn7ybEic3/9I2gj8GdgfmNaUDUoS0N3MNgG7Azeb\n2dPNCVJSZ6CzmW1uznZc8XgVnwNA0mhJD0haK2mdpAcl7ZqXpp+kCZI2SFoYq8p+KWlOIs02VXOS\nvidpdqyiWSLpUUmDJR0BPBiTvR/Xm1PPdnpKujZWI1VIel/ST5twuK8Cwwucg9MlTY9xLo776pqX\n5ixJ78Tqwqck7RvjvCCRZo6k6yT9UNICYG3ivY9KelpSuaQVkv4oqU/i/b6Sbonnd5OkeZL+mHh/\nmKR7JS2NMbwr6ZrE+9tU8UnaR9ITcZ+rJP0lWQUnaVQ8hrMl3SRpjaQFkq6S1JRrxIvx76i05zZW\n8y6P52casAk4K1bpdQZ+HWO8PabvHNeZF78LsySdl3fct8d9flzSrLjNgxLfrf0kTYnn5ZU431uh\ninKNpPcknZu3zZMlPR7P/1pJz0s6Li9N7lj2je+XS3pZ0mH5J0rSlyTNTPxvTJS0Q+L9er8v7Z1n\nUA5J3YEngD2ALwEXAKOBpyXtmEh6O3As8N/AhcBxwKca2PbngO8D1xOqaC4CZgO9gZeAb8WkZxCq\n9D5Rx3YE/D2ufyNwEqFqsH8jDjVnBPB+3vbPBv4GTAVOA64iHONPE2nGAxNi3J8AJgH31LGP84CP\nAV8lniNJhxLO82LgTOAb8ThuS6x3PfBR4FLC+fo+kLzvcichc70QOJFQnda9rgOVNACYAvSKMV0S\n43pcUv7w8dcC62Ns/wf8KL5urFHx7+IYQ4PnNuoF3AHcApwAzCB8JyBU0x5CqMIEuBr4AXBz3Oaz\nwF/yM5QYy7VxXydR+3O/A7gb+CQgQjXwn4CFhON+AbhT0rDEOqMJP6o+G9f7D/BI/GwLHctNMV0F\ncL+kXrkEkq6I7z8NfJzw3V4DbBffT/N9ad/MzKd2PgFXAsvref8rQCWwc2LZMGAz8L04vzfhQnlW\nIk1PYDmhCi237IKYbrs4/zvgr/Xs+5SYflTe8vztHB/nT2vksRvwdUJ1dh/gXMLF4pxEGgFzgdvy\n1v08sBEoi/P3Aa8RR6KOy74d93FBYtkcYBHQI297/waeylt2VFx/7zj/GnBJPcezHji1nvdvB6Yn\n5n9GuLe3fWLZgXGf58b5UXH+zrxtvQJMaOD83g5Mj+e3G7APMBOYR7hIpz23V8YYTq/jM7w4Mb8j\nsAH4cV66h4G38mIzYJ86vlvnJ5adFJfdmli2A7AFuKiOY+8Uj/uxvPVyx3JUYtk+cdkJcb4v4f7q\n9fWc2wa/L+198hKUg3DBesnM3sstMLMFhF+lH42Lxse/DybSbAT+2cC2XwFOitVFByrcB2iKo4CV\nZjapCev+mnChWQvcBdxoZhMS748llKruldQlNwFPAj0ImTPAAcCDFq8UUV3xPGHh/gkA8ZfzIQX2\n8UyMbf+Y9BXgcklflTS2wHZfAX4aq6lGpDj2A4F/mFlNNaOZTSVkoh/NS/uPvPnXCT9UGrI/4Rgq\ngJfjsjPNrJz05xbChfeRFPvbm5D53Ze3/B5grKSBiWUfmNkrdWznicTr2fHvkzXBmK0h3AfdKbcs\nVrHeIekDwo+6LYSahPzPaguh5JrzevybO5+HEH7gFSwNNeL70q55BuUAhgBLCixfQvi1CjAYWJe8\n6EYNNWS4lVBNdTahymSJpGuakFGVEUolTfELQuZyDDAZuFTSSYn3c9WEDxP++XNTrjood79qMNse\nb13Hn38++xHupfxv3j4qCC0Kc/u4GHiAUL32lsL9rnMS2/kUocRyAzA33js5uo4YIN1nm7M6b34z\nIRNpyBuE87sfMMjMPhQzQUh/bgFWWboGDEPi3/zjys33K7CskOTxbi6wLLe8B0C8HzcJ+Ajh8zmS\ncNyPsO15WmuJFoyJ48qlK4t/6/pOp/2+tGveis9B+CfZq8DyQcDK+Hox0EdSj7xMakB9G47/pDcA\nN0gaDnyacN/kA+APjYhxBVsvTI01z8ymA0j6F6EK6heSHomlodwxXsjWEkBS7mK6mG2Pt67jz++v\nszouu5Jwsc63EMDMVhOqJL8uaRyhCvEvkmaY2etm9gFwQbxYHhi3N0nSCDNbUWC7i4CBBZYPYmtj\nhuYqz53fAtKeW9j2nNUld1EfSPhe5OQafqxMLGvJflO7AvsCJ5rZo7mFkprSrD8X9xBCNXm+VN+X\n9s5LUA5CyWZ/SaNzCyTtRPil+ExclLsAnZZI05PQaCIVM5tvZj8jVKfsGRfn/7KsyxPAjpJOSbu/\nOmLYAvww7v/UuPgtQoY5ysymF5hyF5NpwKmxwUbOaaRgZhuA54Hd6tjHNhccM5sBXE74P909771q\nC027ryJUd42sY9cvAMerdkvBAwj3nZ6pY52WlPbcNsZrhPs3Z+UtPxt420IXhdaQy4hq+mJJGgnk\nN5BI4znCPbjzC73ZlO9Le+QlqI6jm6RCLbKeJtxM/g6hNdKPgCpiwwpCKyPM7DVJDwK/jxe7xcBl\nhAtFnZ0xJd1E+EX7PKGF0pHAmLg/CBcwgC9LmkD4NT6zwKYeJ9yMvkvS1YSWdEOAw83sy2lOQMJf\nCZ1LLwcmmVm1pG8Cf5a0PaHKZjOwM6F1Ve5+ys8JF/wJkm5ja6tH6jsHCd8GnpBUTWgxto5wf+Zk\n4Adm9rakZ4D7CRdhi9vfAEyNzY8fI7Tke5vQeu+bhM/ijTr2eT2hddhjkn5OaCH2M0Ip8q8pYm6W\nRpzbxmxzpaRfAVdIqiT8eDqD0NAhvxVfS3oTWABcJ+mHhEY3VxEy4EYxs9UK3QN+EltTPkz4PE8G\nrool5Qa/Ly1wTJnmGVTH0YdtbyoDHGlmUyQdQ7iY/YnQ8moKcIaZJatLLgB+D/yG0JrsRuA9Qj18\nXZ4jXGS/TCglzQa+ZGYPAJjZXEnfIlRrXUK4AIzK34iZmaRPEJoZf4NQtbaQ0OihUeJF86fAHQpP\nO3jOzO6RtJZwv+zzhEz6PcI9q81xvemxGfP/A04nXBgvImSeawvsKn+/z0g6nHBR+zPhHsNc4FG2\n3it5jnCeR8UYXiZUKS2I3QFmEpr5Dyf8OHgeOM7q6AhsZsskHUlopn13PJaHgUtT3u9ptjTntgl+\nRGikcBGham828Jm8xi8tyswqJJ1B+N5PJHxXfwIcQe3GHmm391NJKwmf55eBVcC/CBlR2u9Lu6ba\nDZKcSy+2KnoNeMHMClZVtHeSPkO4eOxsZu83lN45l56XoFxqks4ChhJ+xW9PKBmNITw2qEOQ9HtC\niWkVodXaFcBDnjk51/I8g3KNsYHwUNBdCdUNMwmdRqfWu1b7UkZo+ltGaIl1D+FegXOuhXkVn3PO\ntSOSTiB0Tu8M3BJbzibfv4HQWAlCC9CBZtY3vldF+OEJoXtGqlaqrcUzKOecaydiB/i3Cd0/FhC6\nRpxrZq/Xkf4SYF8z+3ycX29mzRqDrSV5FZ9rEd3U3XrQu9RhFDT8Q+tLHUK9ejbpgeHFsbo6u7EB\nbLGmPjmr9c2btX65mdXbkb0hxx/Z21asrKqZf3FGxWNmdkI9qxwIzM49tix23TidrY9ayncu4aHL\nmeQZlGsRPejNQfU+cad0rp/8XKlDqNde3Yo9vmB6kzb0ajhRCS2u7NtwohK5aPd/zW3uNpatrOTZ\nR4fWzPcaOqehp/fvBMxPzC8ADiqUMHYyHk3i+YNAD4XhWiqBn+W6g5SKZ1DOOZdRhlFhlclF/VV7\nvK+bzezmxHzyKSdbN1PYOcBEM6tKLBthZgsl7Qw8KWmmlWYQTMAzKOecy6xqYFOt/IPlZja+juQQ\nSkzJB8kOo+7n9p0DfC25IPcIJTN7T9IUwrMHS5ZBZbuC2TnnOrBqM8oTUwrTgDEKI2R3I2RC2wwJ\nI2k3whPTn0ss6xefVoKk/oRnDNZ176oovATlnHMZZYhNlr4cYWaVki4mPLOxM2EgxVnx+ZXTE+Op\nnUsYjDKZ6+0B3BSf/deJcA/KMyjnnHPbClV8jWupaGYPkzdEh5n9KG/+ygLr/Qf4UKODbEWeQTnn\nXEZVIzZZx71M+z0o55zLqFDF17VmSkPSCZLekjRb0ncLvH9DHIn5FUlvS1qdeO/8OIrzO5JK/gDo\njps1O+dcxlVbJzZUd0+dPj5J4kYST5KQNCl5L8nMLk2kv4TQUg9JOxI67Y4nNE1/Ma67qiWOpSm8\nBOWccxlV3fgSVM2TJOJ4X7knSdTlXMI4YQDHA4+b2cqYKT0O1PfUilbnJSjnnMuoasSm6loZU0Md\ndZvzJIlC6+7UxNBbhGdQzjmXUWaionbJqaGOus15kkRj1i0Kr+JzzrmMqkZsqO5eM6XQ2CdJ3J2Y\nb8y6ReEZlHPOZVQ1oqK6a82UQpOfJEHo3HtcfKJEP+C4uKxkvIrPOecyymybe1ANpG/6kyTMbKWk\nawiZHMDVZrayRQ6kiTyDcs65jAolqMZdppv6JIm4/Fbg1sZF2Xq8is8BYSTNxOtHJa2WNLmUMTnX\n0VWbKK/uVjOl0VBH3ZjmbEmvS5ol6a7E8qpEJ95tqgaLzUtQrpBfAL2AL5c6EOc6MmtkCSpNR11J\nY4DvAYea2SpJAxOb2Ghm+7RM9M3nJSi3DTN7AlhX6jic6+gMsbm6S82UQpqOul8Cbsw9IcLMlrZo\n0C3IMyjXZJIulDRd0vQtVJQ6HOfanWoTFVVdaiZiR93EdGHeKmk6244Fxkp6VtLzkpJPi+gRt/u8\npI+3+AE1klfxuSaLPdhvBtheO5a0Q59z7ZGZ2FTV4h11uwBjgCMIfZ3+LWlvM1tNxoZ89xKUc85l\nVDWwubpzzZRCms62C4C/m9kWM3sfeIuQYdUa8h2YQnyQbKl4BtUOxacSO+fauHAPqlEZVJqOug8A\nR0LN0O5jgfeyOOS7Z1Dt0wuS7pN0kqRCRf56Sfo3cB9wtKQFko5v+RCdcw0xg81VnWumhtNbJZDr\nqPsGcG+uo66k02Kyx4AVkl4HngIuN7MVhCHfp0t6NS73Id9dqxgLHAN8HvitpHuA283s7bpWMLPt\nEq8Pa/0QnXMNyTWSaIyGOurGp0dcFqdkmswN+e4lqHbIgsfN7Fzgi8D5wFRJT0s6pMThOedSE5VV\nnWumVGs0r6Ouj6jrWpekMuAzwGeBJcAlhHrofQhVd6NLF51zLi0z2FKdvhzRnI66PqKuK5bngO2B\nj5vZyWb2NzOrNLPpwB9KHJtzLiVDVFZ1qplSaE5H3cyNqOsZVPt0hZldY2YLcgsknQVgZj8vXVjO\nucYwgy2VnWsmWrejro+o64riu8C9ecu+R6jec861EYaoql3F12oddVOuW1SeQbUjkk4ETgJ2kvSb\nxFvbA5Wlico512QG1emq9nLSdtR93sy2AO9LynXUXUDItJLrTmlkxC3Kq/jal4XAi8Cm+Dc3TSLU\nLzvn2hCLGVRuSqHJHXXxEXVdazKzV4FXJf1f7LDnnGvTRHVl+r72KUfUzWVErwNVbO2oi4+o61qN\npJnEOuNCD5Aws3HFjsk51wwG1ohm5tD0jrrxvUyNqOsZVPtySqkDcM61IAOqGve0stgq79eEEtQt\nZvazvPcvIAxK+kFc9DszuyW+VwXMjMvnmdlplJBnUO2Imc0tdQzOuZZljcig0nTUje4xs4sLbMJH\n1HWtS9LBkqZJWi9ps6QqSWtLHZdzrpFMqHLrlEKajrpthmdQ7dPvgHOBd4CehOfx/bakETnnGi9X\nxZebWqajLsAnJc2QNFFSslm6j6jrWp+ZzZbU2cyqgNsk/ac197fruA3c/8jU1txFk3VVt1KHUK/1\n1ZtKHUKdduqyudQh1GuLtf9LmKprzbZER90HgbvNrELSV4A7gKPie5kaUbf9f7odU3nsA/GKpGuB\nRUDvEsfknGssAzWukUSDHXVzTcqjPwI/T7xXM6KupCmEEXV9yHfXoj5L+GwvBjYQvrCfLGlEzrkm\nUeXWKYUGO+pKGpKYPY0wsCFZHFHXS1DtUKI13ybgqlLG4pxrOjWyBJWyo+7X4+i6lcBK4IK4+h7A\nTZKqCT9wfURd1/IkHQpcCYwk8Rmb2c6lisk51wQGnaoauUrDHXW/R3h4dP56PqKuK4o/AdcDHwUO\nSEzOuTZGVVunVOkbGFFX0gWSlkl6JU5fTLznI+q6VrfGzB4pdRDOuWay9BkTNK+jro+o64rlKUm/\nkHSIpP1yU6mDcs41jmh0Cao5HXUzN6Kul6Dap4Pi32R/CWNrXwfnXFuwbQmqv6TpifmbzezmxHyh\njroHsa1PSjoceBu41Mzm17Guj6jrWpaZHVnqGJxzLWDbDKo1O+pmbkRdr+JrhyQNkvQnSY/E+T0l\nfaHUcTnnGq9T5dYphVQddc2sIs7+Edg/7brF5hlU+3Q7oR/E0Dj/NvCNkkXjnGsSWXjUUW5Kockd\ndfERdV2R9DezeyV9D2o67zWyN4VzLgsa85/bnI66ZrbSR9R1xbBBUhlbR9c9GFhT2pCcc41WxI66\n8b1MjajrVXzt02WEYv3Okp4F7gQuKW1IzrlGs0Y/i6/BjrqJdGdKMknj4/woSRsTHXj/0DIH0XRe\ngmqfXgfuB8qBdcADhPtQzrk2RECnqvQN6dJ21JXUB/g68ELeJt71EXVda7sT2B34f4SBCscAfy5p\nRM65xrNW66h7DXAt4YHSmeUZVPu0m5l90cyeitOFwNj6VpC0Pv7dR9JzkmbFETc/VZSInXPbiveg\nchMtMKKupH2B4WY2ucAeR0t6WdLTkg5rwSNpEq/ia59elnSwmT0PIOkg4NmU65YDnzOzdyQNJTyP\n6zEzW91awTrnChPQqbJWFV+zOupK6gTcwNYhNpIWEUbUXSFpf+ABSXuZ2dpGB95CPINqnw4CPidp\nXpwfAbwhaSZgZjaurhXN7O3E64WSlgIDAM+gnCs2S984Imqos20fYG9giiSAwcAkSaeZ2XSgAsDM\nXpT0LqHmJflopaLyDKp9apEHPEo6EOhGHUM+x+qFCwGG79S5JXbpnEuyxjWSINFRF/iA0FH3vJrN\nma0B+ufm47Du3zKz6ZIGACvNrErSzoR71+81/yCazjOodigxom6Txd7mfwbON7OCfdjjQypvBtjv\nw91L+swu59ojmaFGZFApO+rW5XDgakmVQBXwFe+o6zJH0vbAQ8AVuftYzrkSMOi0pXG//RrqqJu3\n/IjE678Cf218kK3HW/G5WuLzu+4H7jSz+0odj3MdnaqsZkqVvokddeOy78X13pJ0fAuE3yxegnL5\nziYU9cskXRCXXWBmr5QuJOc6JjXyHlRzOupK2pNwz2ovwoOm/ylprJmV7DmeXoJyAJjZdvHv/5lZ\nVzPbJzF55uRcKRio0mqmFJrTUfd0YIKZVZjZ+8DsuL2S8QzKOeeyyoxOW6prphSa01HXR9R1zjmX\nnqpqZUwNDfnenI66mRtR1zMo55zLKJmhyloZVENPkmhyR90U6xadV/E551xWGVBZvXVqWL0j6prZ\nGjPrb2ajzGwU8DyQe4rEJOAcSd1jR98xwNQWPqJG8RKUc85llRmqTP+so+Z01I3p7iUM11MJfK2U\nLfjAMyjnnMs0pSs51WhqR904/xPgJ42LsPV4FZ9zzmWVGVRWbZ1SaKijrqSvSJoZR819JvZ/8hF1\nnXPONYIZNKKKL2VH3bvM7A8x/WnA9Wx9wLSPqOuccy4FA7ZUbp0a1mBH3bzxnXpT4qbk9fEMyjnn\nsipXgspNDUvV2VbS1+J4T9cSHnmUk6kRdT2Dcs65rDLDKitrJhoe8j1VZ1szu9HMdgG+A1wRF+dG\n1N0XuAy4K45sUDJ+D8o557LKLL9qr7kddfNNAH4fdmUVZGxEXS9BOedcVplhm7fUTCnU21EXQNKY\nxOzJwDtx+YDYyAIfUdc551y9zAyrTJUx5dKn6ah7saRjgC3AKuD8uHrmRtSVWWYbcLg2RNIyoNlD\nzUf9geUttK3WkOX4shwbZDu+lo5tpJkNaM4GJD1KiCtnuZmdUFf69sYzKJc5kqY3UM9eUlmOL8ux\nQbbjy3JsHZXfg3LOOZdJnkE555zLJM+gXBbd3HCSkspyfFmODbIdX5Zj65D8HpRzzrlM8hKUc865\nTPIMyjnnXCZ5BuWKKsVYNd0l3RPff0HSqLi8TNJTktZL+l0G4jxc0kuSKiWd2drx5O37VklLJb1W\nx/uS9JsY+wxJ+5UiJkk7Snpc0jvxb7861j0/pnlH0vmF0rRmPGnPl6T94zhKs2P6Qs+9cy3IMyhX\nNImxak4E9gTOzQ2WlvAFYJWZ7QrcAPw8Lt8E/BD4VkbinAdcANzV2vEUcDtbx+8p5ETCY2rGABcS\nn7VWgpi+CzxhZmOAJ+J8LZJ2BH4MHEQYKuLHdWVkrRhP2vP1+/h+Lm2H6TBbKp5BuWJqcKyaOH9H\nfD0ROFqSzGyDmT1DyKhKHqeZzTGzGUDjxuNuAWb2L6C+R9CcDtxpwfNAX0lDShBT8rO8A/h4gVWP\nBx43s5Vmtgp4nBa48DcyngbPV5zf3syes9Cy7M46jse1IM+gXDGlGaumJo2ZVQJrgLKiRFcghqjg\nmDoZlpX4B5nZIoD4d2CBNMWMta540n4vFxQpThd5BuWKKc1YNanGs2llWYihOdpS/FmIta18Lzsc\nz6BcMaUZq6YmjaQuwA7UX53VGho7pk7WZCX+Jbmqsvh3aYE0xYy1rnjSfi+HFSlOF3kG5YqpwbFq\n4nyuJdeZwJNW/N7kaeLMsknA52LrtIOBNbmqrRLEkfsszwf+XiDNY8BxkvrFxhHHxWXFjKfB8xXn\n10k6OLbe+xyFj8e1JDPzyaeiTcBJwNvAu8AP4rKrgdPi6x7AfcBsYCqwc2LdOYTS1HrCL9o9Sxjn\nATGGDcAKYFYRz+HdhBut3WMAAAFLSURBVOG5t8QYvgB8hTB+D4TqqBtj7DOB8SWKqYzQWu6d+HfH\nmHY8cEti3c/Hz3s28F8liKfO8wW8kng9Hngtpvsd8Uk8PrXe5I86cs45l0lexeeccy6TPINyzjmX\nSZ5BOeecyyTPoJxzzmWSZ1DOOecyyTMo59oBSYMlTZD0rqTXJT0saWyp43KuOTyDcq6Nix1H7wem\nmNkuZrYn8H1gUGkjc655upQ6AOdcsx0JbDGzP+QWmNkrJYzHuRbhJSjn2r69gRdLHYRzLc0zKOec\nc5nkGZRzbd8sYP9SB+FcS/MMyrm270mgu6Qv5RZIOkDSx0oYk3PN5g+Lda4dkDQU+BWhJLWJ8OT3\nb5jZO6WMy7nm8AzKOedcJnkVn3POuUzyDMo551wmeQblnHMukzyDcs45l0meQTnnnMskz6Ccc85l\nkmdQzjnnMun/A3WDpVfKlRlyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1b771208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "mnb_fbeta_scores = []\n",
    "\n",
    "for alpha in np.arange(0.2, 1.2, 0.2):\n",
    "    mnb = MultinomialNB(alpha=alpha)\n",
    "    mnb.fit(train_X, train_y)\n",
    "    predictions = mnb.predict(dev_X)\n",
    "    mnb_fbeta_scores.append(fbeta_score(dev_y, predictions, beta=0.5))\n",
    "    \n",
    "print(\"mnb fbeta scores\", mnb_fbeta_scores)\n",
    "    \n",
    "lr_fbeta_scores = []\n",
    "\n",
    "for penalty in ['l1', 'l2']:\n",
    "    for C in [0.01, 0.1, 1.00, 10.0, 100.00]:\n",
    "        lr = LogisticRegression(C=C, penalty=penalty)\n",
    "        lr.fit(train_X, train_y)\n",
    "        predictions = lr.predict(dev_X)\n",
    "        lr_fbeta_scores.append(fbeta_score(dev_y, predictions, beta=0.5))\n",
    "\n",
    "print(\"lr fbeta scores:\", lr_fbeta_scores)\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(np.arange(0.2, 1.2, 0.2), mnb_fbeta_scores)\n",
    "plt.title('Multinomial Naive Bayes Performance', fontsize=15)\n",
    "plt.xlabel(r'$\\alpha$')\n",
    "plt.ylabel('fbeta score')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "lr_fbeta_scores = np.asarray(lr_fbeta_scores)\n",
    "lr_fbeta_scores = lr_fbeta_scores.reshape((2,5))\n",
    "plt.yticks(range(2),['l1', 'l2'])\n",
    "plt.xticks(range(5),[0.01, 0.1, 1.00, 10.0, 100.00])\n",
    "plt.imshow(lr_fbeta_scores)\n",
    "plt.xlabel('C')\n",
    "plt.title('Logistic Regression Performance', fontsize=15)\n",
    "plt.ylabel('penalty')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Using the best settings you have found, compare the two classifiers based on performance in the test set. Print out both accuracy and macroaveraged f-score for each classifier. Be sure to label your output. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def check_results(predictions, classifications):\n",
    "    print(\"accuracy\")\n",
    "    print(accuracy_score(classifications,predictions))\n",
    "    print(classification_report(classifications,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Multinomial Naive Bayes performance:\n",
      "accuracy\n",
      "0.766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.77      0.76       491\n",
      "          1       0.77      0.76      0.77       509\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1000\n",
      "\n",
      "----------------------------------\n",
      "optimal Logistic Regression performance:\n",
      "accuracy\n",
      "0.739\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.78      0.75       491\n",
      "          1       0.77      0.70      0.73       509\n",
      "\n",
      "avg / total       0.74      0.74      0.74      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"optimal Multinomial Naive Bayes performance:\")\n",
    "mnb = MultinomialNB(alpha=1.0)\n",
    "mnb.fit(train_X, train_y)\n",
    "predictions = mnb.predict(test_X)\n",
    "check_results(predictions, test_y)\n",
    "print(\"----------------------------------\")\n",
    "print(\"optimal Logistic Regression performance:\")\n",
    "lr = LogisticRegression(C=1.0, penalty='l1')\n",
    "lr.fit(train_X, train_y)\n",
    "predictions = lr.predict(test_X)\n",
    "check_results(predictions, test_y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
