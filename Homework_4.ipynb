{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Language Modelling in Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Due date</b>: 11pm, Wednesday May 2nd\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -20% per day\n",
    "\n",
    "<b>Marks</b>: 5% of mark for class\n",
    "\n",
    "<b>Overview</b>: In this homework, you'll be creating an 'artificial intelligence' player for the classic Hangman word guessing game. You will need to implement several different automatic strategies based on character level language models, ranging from unigram approaches to higher over n-gram models. Your objective is to create an automatic player which makes the fewest mistakes.\n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn, and Gensim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks.  \n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
    "\n",
    "<b>Extra credit</b>: Each homework has a task which is optional with respect to getting full marks on the assignment, but that can be used to offset any points lost on this or any other homework assignment (but not the final project or the exam). We recommend you skip over this step on your first pass, and come back if you have time: the amount of effort required to receive full marks (1 point) on an extra credit question will be substantially more than earning the same amount of credit on other parts of the homework.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hangman Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <a href=\"https://en.wikipedia.org/wiki/Hangman_(game)\">Hangman game</a> is a simple game whereby one person thinks of a word, which they keep secret from their opponent, who tries to guess the word one character at a time. The game ends when the opponent makes more than a fixed number of incorrect guesses, or they figure out the secret word before then (in which case they *win*). \n",
    "\n",
    "Here's a simple version of the game, and a method allowing interactive play. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allowing better python 2 & python 3 compatibility \n",
    "from __future__ import print_function \n",
    "\n",
    "def hangman(secret_word, guesser, max_mistakes=8, verbose=True, **guesser_args):\n",
    "    \"\"\"\n",
    "        secret_word: a string of lower-case alphabetic characters, i.e., the answer to the game\n",
    "        guesser: a function which guesses the next character at each stage in the game\n",
    "            The function takes a:\n",
    "                mask: what is known of the word, as a string with _ denoting an unknown character\n",
    "                guessed: the set of characters which already been guessed in the game\n",
    "                guesser_args: additional (optional) keyword arguments, i.e., name=value\n",
    "        max_mistakes: limit on length of game, in terms of allowed mistakes\n",
    "        verbose: be chatty vs silent\n",
    "        guesser_args: keyword arguments to pass directly to the guesser function\n",
    "    \"\"\"\n",
    "    secret_word = secret_word.lower()\n",
    "    mask = ['_'] * len(secret_word)\n",
    "    guessed = set()\n",
    "    if verbose:\n",
    "        print(\"Starting hangman game. Target is\", ' '.join(mask), 'length', len(secret_word))\n",
    "    \n",
    "    mistakes = 0\n",
    "    while mistakes < max_mistakes:\n",
    "        if verbose:\n",
    "            print(\"You have\", (max_mistakes-mistakes), \"attempts remaining.\")\n",
    "        guess = guesser(mask, guessed, **guesser_args)\n",
    "\n",
    "        if verbose:\n",
    "            print('Guess is', guess)\n",
    "        if guess in guessed:\n",
    "            if verbose:\n",
    "                print('Already guessed this before.')\n",
    "            mistakes += 1\n",
    "        else:\n",
    "            guessed.add(guess)\n",
    "            if guess in secret_word:\n",
    "                for i, c in enumerate(secret_word):\n",
    "                    if c == guess:\n",
    "                        mask[i] = c\n",
    "                if verbose:\n",
    "                    print('Good guess:', ' '.join(mask))\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Sorry, try again.')\n",
    "                mistakes += 1\n",
    "                \n",
    "        if '_' not in mask:\n",
    "            if verbose:\n",
    "                print('Congratulations, you won.')\n",
    "            return mistakes\n",
    "        \n",
    "    if verbose:\n",
    "        print('Out of guesses. The word was', secret_word)    \n",
    "    return mistakes\n",
    "\n",
    "def human(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    simple function for manual play\n",
    "    \"\"\"\n",
    "    print('Enter your guess:')\n",
    "    try:\n",
    "        return raw_input().lower().strip() # python 3\n",
    "    except NameError:\n",
    "        return input().lower().strip() # python 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play the game interactively using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hangman('tea', human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Instructions</b>: We will be using the words occurring in the *Brown* corpus for *training* an artificial intelligence guessing algorithm, and for *evaluating* the quality of the method. Note that we are intentionally making the hangman game hard, as the AI will need to cope with test words that it has not seen before, hence it will need to learn generalisable patterns of characters to make reasonable predictions.\n",
    "\n",
    "Your first task is to compute the unique word types occurring in the *Brown* corpus, using `nltk.corpus.Brown`, selecting only words that are entirely comprised of alphabetic characters, and lowercasing the words. Finally, randomly shuffle (`numpy.random.shuffle`) this collection of word types, and split them into disjoint training and testing sets. The test set should contain 1000 word types, and the rest should be in the training set. Your code should print the sizes of the training and test sets.\n",
    "\n",
    "Feel free to test your own Hangman performance using `hangman(numpy.random.choice(test_set), human, 8, True)`. It is surprisingly difficult (and addictive)!\n",
    "\n",
    "(0.5 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set()\n",
    "for word in brown.words():\n",
    "    if re.match(r\"^[a-zA-Z]+$\", word):\n",
    "        word = word.lower()\n",
    "        if word not in word_set:\n",
    "            word_set.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test set is: 1000\n",
      "Length of train set is: 39234\n"
     ]
    }
   ],
   "source": [
    "word_list = np.asarray(list(word_set))\n",
    "np.random.shuffle(word_list)\n",
    "\n",
    "test_set = word_list[:1000]\n",
    "train_set = word_list[1000:]\n",
    "\n",
    "print(\"Length of test set is:\", len(test_set))\n",
    "print(\"Length of train set is:\", len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hangman(np.random.choice(test_set), human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: To set a baseline, your first *AI* attempt will be a trivial random method. For this you should implement a guessing method, similar to the `human` method above, i.e., using the same input arguments and returning a character. Your method should randomly choose a character from the range `'a'...'z'` after excluding the characters that have already been guessed in the current game (all subsequent AI approaches should also exclude previous guesses). You might want to use `numpy.random.choice` for this purpose.\n",
    "\n",
    "To measure the performance of this (and later) techiques, implement a method that measures the average number of mistakes made by this technique over all the words in the `test_set`. You will want to turn off the printouts for this, using the `verbose=False` option, and increase the cap on the game length to `max_mistakes=26`. Print the average number of mistakes for the random AI, which will become a baseline for the following steps.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_guesser(mask, guessed, **kwargs):\n",
    "    alphabet = {chr(i) for i in range(ord('a'), ord('z')+1)}\n",
    "    return np.random.choice(list(alphabet - guessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hangman game. Target is _ _ _ _ _ _ _ length 7\n",
      "You have 26 attempts remaining.\n",
      "Guess is s\n",
      "Good guess: _ _ _ _ _ _ s\n",
      "You have 26 attempts remaining.\n",
      "Guess is d\n",
      "Sorry, try again.\n",
      "You have 25 attempts remaining.\n",
      "Guess is b\n",
      "Sorry, try again.\n",
      "You have 24 attempts remaining.\n",
      "Guess is h\n",
      "Sorry, try again.\n",
      "You have 23 attempts remaining.\n",
      "Guess is e\n",
      "Good guess: _ _ _ _ e _ s\n",
      "You have 23 attempts remaining.\n",
      "Guess is j\n",
      "Sorry, try again.\n",
      "You have 22 attempts remaining.\n",
      "Guess is a\n",
      "Good guess: _ a _ _ e _ s\n",
      "You have 22 attempts remaining.\n",
      "Guess is c\n",
      "Sorry, try again.\n",
      "You have 21 attempts remaining.\n",
      "Guess is l\n",
      "Good guess: _ a _ _ e l s\n",
      "You have 21 attempts remaining.\n",
      "Guess is u\n",
      "Sorry, try again.\n",
      "You have 20 attempts remaining.\n",
      "Guess is r\n",
      "Good guess: _ a r _ e l s\n",
      "You have 20 attempts remaining.\n",
      "Guess is p\n",
      "Sorry, try again.\n",
      "You have 19 attempts remaining.\n",
      "Guess is g\n",
      "Sorry, try again.\n",
      "You have 18 attempts remaining.\n",
      "Guess is y\n",
      "Sorry, try again.\n",
      "You have 17 attempts remaining.\n",
      "Guess is i\n",
      "Sorry, try again.\n",
      "You have 16 attempts remaining.\n",
      "Guess is t\n",
      "Sorry, try again.\n",
      "You have 15 attempts remaining.\n",
      "Guess is o\n",
      "Sorry, try again.\n",
      "You have 14 attempts remaining.\n",
      "Guess is m\n",
      "Good guess: m a r _ e l s\n",
      "You have 14 attempts remaining.\n",
      "Guess is n\n",
      "Sorry, try again.\n",
      "You have 13 attempts remaining.\n",
      "Guess is v\n",
      "Good guess: m a r v e l s\n",
      "Congratulations, you won.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hangman(np.random.choice(test_set), random_guesser, 26, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(guesser, max_mistakes=26, **guesser_args):\n",
    "    mistake_num = 0\n",
    "    count = 0.0\n",
    "    \n",
    "    for test_word in test_set:\n",
    "        mistake_num += \\\n",
    "            hangman(test_word, guesser, max_mistakes, False, **guesser_args)\n",
    "        count += 1\n",
    "    \n",
    "    return mistake_num / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of mistakes for the random AI is: 16.746\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of mistakes for the random AI is:\", \n",
    "     measure_performance(random_guesser))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** As your first real AI, you should train a *unigram* model over the training set.  This requires you to find the frequencies of characters over all training words. Using this model, you should write a guess function that returns the character with the highest probability, after aggregating (summing) the probability of each blank character in the secret word. Print the average number of mistakes the unigram method makes over the test set. Remember to exclude already guessed characters, and use the same evaluation method as above, so the results are comparable. (Hint: it should be much lower than for random).\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_counts(words):\n",
    "    unigram_counts = Counter()\n",
    "\n",
    "    # collect unigram statistics\n",
    "    for word in words:\n",
    "        for char in word:\n",
    "            unigram_counts[char] += 1\n",
    "        \n",
    "    return unigram_counts\n",
    "\n",
    "\n",
    "def unigram_guesser(mask, guessed, **kwargs):\n",
    "    for (char, _) in kwargs[\"unigrams\"].most_common():\n",
    "        if char not in guessed:\n",
    "            return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'e': 35251, 'i': 26012, 'a': 24397, 's': 23729, 'n': 22591, 'r': 22074, 't': 20672, 'o': 19219, 'l': 16714, 'c': 12623, 'd': 12281, 'u': 10162, 'm': 8686, 'g': 8605, 'p': 8534, 'h': 7337, 'b': 5749, 'y': 5355, 'f': 4228, 'v': 3442, 'k': 2972, 'w': 2797, 'z': 1020, 'x': 832, 'j': 641, 'q': 527})\n"
     ]
    }
   ],
   "source": [
    "unigrams = get_counts(train_set)\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of mistakes for the unigram AI is: 10.522\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of mistakes for the unigram AI is:\", \n",
    "      measure_performance(unigram_guesser, unigrams=unigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** The length of the secret word is an important clue that we might exploit. Different length words tend to have different distributions over characters, e.g., short words are less likely to have suffixes or prefixes. Your job now is to incorporate this idea by conditioning the unigram model on the length of the secret word, i.e., having *different* unigram models for each length of word. You will need to be a little careful at test time, to be robust to the (unlikely) situation that you encounter a word length that you didn't see in training. Create another AI guessing function using this new model, and print its test performance.   \n",
    "\n",
    "(0.5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_conditional_counts(words):\n",
    "    unigram_counts = defaultdict(Counter)\n",
    "    # collect unigram statistics\n",
    "    for word in words:\n",
    "        for char in word:\n",
    "            unigram_counts[len(word)][char] += 1\n",
    "        \n",
    "    return unigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conditional_unigrams = get_conditional_counts(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_unigram_guesser(mask, guessed, **kwargs):\n",
    "    alphabet = {chr(i) for i in range(ord('a'), ord('z')+1)}\n",
    "    if not len(mask) in kwargs[\"unigrams\"]:\n",
    "        return np.random.choice(list(alphabet - guessed))\n",
    "    \n",
    "    for (char, _) in kwargs[\"unigrams\"][len(mask)].most_common():\n",
    "        if char not in guessed:\n",
    "            return char\n",
    "\n",
    "    return np.random.choice(list(alphabet - guessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of mistakes for the unigram AI is: 10.419\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of mistakes for the unigram AI is:\", \n",
    "      measure_performance(conditional_unigram_guesser, unigrams=conditional_unigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Now for the main challenge, using a *ngram* language model over characters. The order of characters is obviously important, yet this wasn't incorporated in any of the above models. Knowing that the word has the sequence `n _ s s` is a pretty strong clue that the missing character might be `e`. Similarly the distribution over characters that start or end a word are highly biased (e.g., toward common prefixes and suffixes, like *un-*, *-ed* and *-ly*).\n",
    "\n",
    "Your job is to develop a *ngram* language model over characters, train this over the training words (being careful to handle the start of each word properly, e.g., by padding with sentinel symbols.) You should use linear interpolation to smooth between the higher order and lower order models, and you will have to decide how to weight each component. \n",
    "\n",
    "Your guessing AI algorithm should apply your language model to each blank position in the secret word by using as much of the left context as is known. E.g., in `_ e c _ e _ _` we know the full left context for the first blank (context=start of word), we have a context of two characters for the second blank (context=ec), one character for the second last blank (context=e), and no known context for the last one. If we were using a *n=3* order model, we would be able to apply it to the first and second blanks, but would only be able to use the bigram or unigram distributions for the subsequent blanks. As with the unigram model, you should sum over the probability distributions for each blank to find the expected count for each character type, then select the  character with the highest expected count.\n",
    "\n",
    "Implement the ngram method for *n=3,4* and *5* and evaluate each of these three models over the test set. Do you see any improvement over the unigram methods above?\n",
    "\n",
    "(2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_counts(words):\n",
    "    \n",
    "    def convert_word(word, n):\n",
    "        return \"*\" * (n - 1) + word\n",
    "    \n",
    "    counter_dict = dict()\n",
    "    \n",
    "    for n in range(2, 7):\n",
    "        counter_dict[n] = defaultdict(Counter)\n",
    "        for word in words:\n",
    "            word = convert_word(word, n)\n",
    "            ngram_list = [word[index:index+n] for index, _ in enumerate(word[:-n + 1])]\n",
    "            for ngram in ngram_list:\n",
    "                first, second = ngram[:-1], ngram[-1]\n",
    "                counter_dict[n][first][second] += 1\n",
    "    \n",
    "    return counter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = get_ngram_counts(train_set)\n",
    "ngrams[1] = unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_interp(prev_chars, char, ngram_counts, lambdas):\n",
    "    n = len(prev_chars) + 1\n",
    "    \n",
    "    if n == 1:\n",
    "        return ngram_counts[n][char] / float(sum(ngram_counts[n].values()))\n",
    "\n",
    "    total_count = float(sum(ngram_counts[n][prev_chars].values()))\n",
    "    interp_prob = 0.0\n",
    "    \n",
    "    if total_count != 0:\n",
    "        interp_prob = ngram_counts[n][prev_chars][char] / total_count\n",
    "    \n",
    "    return lambdas * interp_prob + \\\n",
    "        (1 - lambdas) * get_interp(prev_chars[1:], char, ngram_counts, lambdas)\n",
    "\n",
    "    \n",
    "def get_char_log_prob_interp(chars_list, char, n, ngram_counts, lambdas):\n",
    "    log_prob = 0\n",
    "    \n",
    "    for index, chars in enumerate(chars_list):\n",
    "        if len(chars) >= n:\n",
    "            chars = chars[-n + 1:]\n",
    "        log_prob += math.log(get_interp(chars, char, ngram_counts, lambdas) + 1)\n",
    "    \n",
    "    return log_prob\n",
    "      \n",
    "    \n",
    "def trigram_guesser(mask, guessed, **kwargs):\n",
    "    mask = \"**\" + \"\".join(mask)\n",
    "    chars_list = mask.split(\"_\")\n",
    "    chars_list = chars_list[:-1]\n",
    "    char_prob = dict()\n",
    "    \n",
    "    for i in [chr(c) for c in range(ord('a'), ord('z')+1)]:\n",
    "        if not i in guessed:\n",
    "            char_prob[i] = \\\n",
    "                get_char_log_prob_interp(chars_list, i, 3, kwargs[\"ngrams\"], 0.9)\n",
    "    return max(char_prob, key=char_prob.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of mistakes for the trigram AI is: 8.234\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of mistakes for the trigram AI is:\", \n",
    "      measure_performance(trigram_guesser, ngrams=ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of mistakes for the fourgram AI is: 7.668\n"
     ]
    }
   ],
   "source": [
    "def fourgram_guesser(mask, guessed, **kwargs):\n",
    "    mask = \"***\" + \"\".join(mask)\n",
    "    chars_list = mask.split(\"_\")\n",
    "    chars_list = chars_list[:-1]\n",
    "    char_prob = dict()\n",
    "    \n",
    "    for i in [chr(c) for c in range(ord('a'), ord('z')+1)]:\n",
    "        if not i in guessed:\n",
    "            char_prob[i] = \\\n",
    "                get_char_log_prob_interp(chars_list, i, 4, kwargs[\"ngrams\"], 0.9)\n",
    "    return max(char_prob, key=char_prob.get)\n",
    "\n",
    "print(\"Average number of mistakes for the fourgram AI is:\", \n",
    "      measure_performance(fourgram_guesser, ngrams=ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of mistakes for the fivegram AI is: 7.484\n"
     ]
    }
   ],
   "source": [
    "def fivegram_guesser(mask, guessed, **kwargs):\n",
    "    mask = \"****\" + \"\".join(mask)\n",
    "    chars_list = mask.split(\"_\")\n",
    "    chars_list = chars_list[:-1]\n",
    "    char_prob = dict()\n",
    "    \n",
    "    for i in [chr(c) for c in range(ord('a'), ord('z')+1)]:\n",
    "        if not i in guessed:\n",
    "            char_prob[i] = \\\n",
    "                get_char_log_prob_interp(chars_list, i, 5, kwargs[\"ngrams\"], 0.9)\n",
    "    return max(char_prob, key=char_prob.get)\n",
    "\n",
    "print(\"Average number of mistakes for the fivegram AI is:\", \n",
    "      measure_performance(fivegram_guesser, ngrams=ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Improving the AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** To get the extra credit, you should try to develop a more effective AI for hangman. Feel free to engage your creativity here! Possibilities include better conditioning on the length of the word and the parts that are known, fancier smoothing methods, using backwards ngram models, or a fancier inference algorithm like forward-back using a HMM. Ensure you report the test performanceÂ of your method.\n",
    "\n",
    "You will be marked based on the ambitionÂ of your approach and on its accuracy. If you have tried some truly spectacular method but it didn't really work, then please include your implementation and an explanation, which will still attract marks for ambition.\n",
    "\n",
    "(1 bonus mark) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try sixgram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of mistakes for the sixgram AI is: 7.456\n"
     ]
    }
   ],
   "source": [
    "def sixgram_guesser(mask, guessed, **kwargs):\n",
    "    mask = \"****\" + \"\".join(mask)\n",
    "    chars_list = mask.split(\"_\")\n",
    "    chars_list = chars_list[:-1]\n",
    "    char_prob = dict()\n",
    "    \n",
    "    for i in [chr(c) for c in range(ord('a'), ord('z')+1)]:\n",
    "        if not i in guessed:\n",
    "            char_prob[i] = \\\n",
    "                get_char_log_prob_interp(chars_list, i, 6, kwargs[\"ngrams\"], 0.9)\n",
    "    return max(char_prob, key=char_prob.get)\n",
    "\n",
    "print(\"Average number of mistakes for the sixgram AI is:\", \n",
    "      measure_performance(sixgram_guesser, ngrams=ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Backoff to smooth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backoff(prev_chars, char, ngram_counts, lambdas):\n",
    "    n = len(prev_chars) + 1\n",
    "    \n",
    "    if n == 1:\n",
    "        return ngram_counts[n][char] / float(sum(ngram_counts[n].values()))\n",
    "\n",
    "    total_count = float(sum(ngram_counts[n][prev_chars].values()))\n",
    "    \n",
    "    if total_count != 0:\n",
    "        return lambdas * ngram_counts[n][prev_chars][char] / total_count\n",
    "    else:\n",
    "        return (1 - lambdas) * get_interp(prev_chars[1:], char, ngram_counts, lambdas)\n",
    "\n",
    "    \n",
    "def get_char_log_prob_backoff(chars_list, char, n, ngram_counts, lambdas):\n",
    "    log_prob = 0\n",
    "    \n",
    "    for index, chars in enumerate(chars_list):\n",
    "        if len(chars) >= n:\n",
    "            chars = chars[-n + 1:]\n",
    "        log_prob += math.log(get_backoff(chars, char, ngram_counts, lambdas) + 1)\n",
    "    \n",
    "    return log_prob\n",
    "\n",
    "\n",
    "def fivegram_backoff_guesser(mask, guessed, **kwargs):\n",
    "    mask = \"****\" + \"\".join(mask)\n",
    "    chars_list = mask.split(\"_\")\n",
    "    chars_list = chars_list[:-1]\n",
    "    char_prob = dict()\n",
    "    \n",
    "    for i in [chr(c) for c in range(ord('a'), ord('z')+1)]:\n",
    "        if not i in guessed:\n",
    "            char_prob[i] = \\\n",
    "                get_char_log_prob_backoff(chars_list, i, 5, kwargs[\"ngrams\"], 0.9)\n",
    "    return max(char_prob, key=char_prob.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of mistakes for the fivegram (backoff) AI is: 7.842\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of mistakes for the fivegram (backoff) AI is:\", \n",
    "      measure_performance(fivegram_backoff_guesser, ngrams=ngrams))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
